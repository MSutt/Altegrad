{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTEGRAD Challenge - Classification\n",
    "\n",
    "*Abderrahim AIT-AZZI, SÃ©bastien OHLEYER, Mickael SUTTON*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Performing grid search\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from log import _check_log_directory,_initialise_model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to make log directory at ./log/29-01-2018_19-19-06\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/\"\n",
    "log_dir = './log'\n",
    "\n",
    "#initialize model directory\n",
    "log_name = (datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "log_filepath = os.path.join(log_dir,log_name,'lighgb.csv')\n",
    "_check_log_directory(os.path.join(log_dir,log_name))\n",
    "_initialise_model_log(log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Glove features\n",
    "features_train = pd.read_csv(data_dir+'train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv(data_dir+'test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_train= features_train.drop(['question1', 'question2'], axis=1)\n",
    "features_test = features_test.drop(['id','qid1','qid2','question1', 'question2'], axis=1)\n",
    "data_train = pd.read_csv(data_dir+'train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "\n",
    "# Pagerank features\n",
    "pagerank_feats_train = pd.read_csv(data_dir+\"train_pagerank.csv\", sep=',')\n",
    "pagerank_feats_test = pd.read_csv(data_dir+\"test_pagerank.csv\", sep=',')\n",
    "\n",
    "# Question frequency\n",
    "train_question_freq = pd.read_csv(data_dir+'train_question_freq.csv', sep=',', index_col=0)\n",
    "test_question_freq = pd.read_csv(data_dir+'test_question_freq.csv', sep=',', index_col=0)\n",
    "\n",
    "# Intersection of questions\n",
    "train_question_inter= pd.read_csv(data_dir+'train_question_inter.csv', sep=',', index_col=0)\n",
    "test_question_inter = pd.read_csv(data_dir+'test_question_inter.csv', sep=',', index_col=0)\n",
    "\n",
    "# K-cores\n",
    "train_kcores = pd.read_csv(data_dir+'train_kcores.csv', sep=',', index_col=0)\n",
    "test_kcores = pd.read_csv(data_dir+'test_kcores.csv', sep=',', index_col=0)\n",
    "\n",
    "# question K-cores\n",
    "train_question_kcores = pd.read_csv(data_dir+'train_question_kcores.csv', sep=',', index_col=0)\n",
    "test_question_kcores = pd.read_csv(data_dir+'test_question_kcores.csv', sep=',', index_col=0)\n",
    "\n",
    "# TF-IDF\n",
    "train_tfidf = pd.read_csv(data_dir+'train_tfidf.csv', sep=',', index_col=0)\n",
    "test_tfidf = pd.read_csv(data_dir+'test_tfidf.csv', sep=',', index_col=0)\n",
    "\n",
    "# Graph features\n",
    "train_graph_feat = pd.read_csv(data_dir+'train_graph_feat.csv', sep=',', index_col=0)\n",
    "test_graph_feat = pd.read_csv(data_dir+'test_graph_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# Bigram feature\n",
    "train_bigram_feat = pd.read_csv(data_dir+'train_2gram_feat.csv', sep=',', index_col=0)\n",
    "test_bigram_feat = pd.read_csv(data_dir+'test_2gram_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# 3gram feature\n",
    "train_3gram_feat = pd.read_csv(data_dir+'train_3gram_feat.csv', sep=',', index_col=0)\n",
    "test_3gram_feat = pd.read_csv(data_dir+'test_3gram_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# spaCy feature\n",
    "train_spacy_feat = pd.read_csv(data_dir+'train_spacy_features.csv', sep=',', index_col=0)\n",
    "test_spacy_feat = pd.read_csv(data_dir+'test_spacy_features.csv', sep=',', index_col=0)\n",
    "\n",
    "# Graph features2 NE PAS PRENDRE !!!\n",
    "#train_graph_feat2 = pd.read_csv(data_dir+'train_graph_feat2.csv', sep=',', index_col=0)\n",
    "#test_graph_feat2 = pd.read_csv(data_dir+'test_graph_feat2.csv', sep=',', index_col=0)\n",
    "\n",
    "# Word features\n",
    "train_word_feat = pd.read_csv(data_dir+'train_word_feat.csv', sep=',', index_col=0)\n",
    "test_word_feat = pd.read_csv(data_dir+'test_word_feat.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pagerank features\n",
    "features_train[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_train[[\"q1_pr\",\"q2_pr\"]]\n",
    "features_test[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_test[[\"q1_pr\",\"q2_pr\"]]\n",
    "\n",
    "# Add question frequency features\n",
    "features_train[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=train_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "features_test[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=test_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "\n",
    "# Add intersection of questions features\n",
    "features_train['q1_q2_intersect']=train_question_inter['q1_q2_intersect']\n",
    "features_test['q1_q2_intersect']=test_question_inter['q1_q2_intersect']\n",
    "\n",
    "# Add K-cores\n",
    "features_train[[\"core1\",\"core2\",\"core3\"]] = train_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "features_test[[\"core1\",\"core2\",\"core3\"]] = test_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "\n",
    "# Add question K-cores features\n",
    "features_train[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "                'q1_q2_kcores_diff_normed']]=train_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "features_test[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "               'q1_q2_kcores_diff_normed']]=test_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "\n",
    "# Add TF-IDF features\n",
    "features_train[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=train_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "features_test[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=test_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "\n",
    "# Add graph features\n",
    "features_train[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']] = train_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']]\n",
    "features_test[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']] = test_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']]\n",
    "\n",
    "# Add bigram features\n",
    "features_train[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']] = train_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "features_test[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']] = test_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "\n",
    "# Add 3gram features\n",
    "features_train[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']] = train_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "features_test[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']] = test_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "\n",
    "# Add spaCy features\n",
    "features_train[['spacy_similarity']] = train_spacy_feat[['spacy_similarity']]\n",
    "features_test[['spacy_similarity']] = test_spacy_feat[['spacy_similarity']]\n",
    "\n",
    "# Add graph features2\n",
    "#features_train[['shortest_path']] = train_graph_feat2[['shortest_path']]\n",
    "#features_test[['shortest_path']] = test_graph_feat2[['shortest_path']]\n",
    "\n",
    "# Add graph features\n",
    "features_train[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]=train_word_feat[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]\n",
    "features_test[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]=test_word_feat[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= features_train.drop(['is_duplicate'],axis=1)\n",
    "X_test = features_test\n",
    "X_train=X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_train=X_train.fillna(value=0)\n",
    "X_test=X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test=X_test.fillna(value=0)\n",
    "Y_train = data_train[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['len_q1', 'len_q2', 'diff_len', 'len_char_q1', 'len_char_q2',\n",
       "       'len_word_q1', 'len_word_q2', 'common_words', 'fuzz_qratio',\n",
       "       'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
       "       'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio',\n",
       "       'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cosine_distance',\n",
       "       'cityblock_distance', 'jaccard_distance', 'canberra_distance',\n",
       "       'euclidean_distance', 'minkowski_distance', 'braycurtis_distance',\n",
       "       'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec', 'q1_pr', 'q2_pr',\n",
       "       'q1_hash', 'q2_hash', 'q1_freq', 'q2_freq', 'q1_q2_intersect', 'core1',\n",
       "       'core2', 'core3', 'q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio',\n",
       "       'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed', 'word_match',\n",
       "       'tfidf_wm', 'tfidf_wm_stops', 'jaccard', 'wc_diff', 'wc_ratio',\n",
       "       'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unq_stop',\n",
       "       'wc_ratio_unique_stop', 'same_start', 'char_diff', 'char_diff_unq_stop',\n",
       "       'total_unique_words', 'total_unq_words_stop', 'char_ratio', 'q1_neigh',\n",
       "       'q2_neigh', 'common_neigh', 'distinct_neigh', 'clique_size',\n",
       "       'shortest_path', 'bigram_coocurence', 'bigram_distinct',\n",
       "       'bigram_nostpwrd_coocurence', 'bigram_nostpwrd_distinct',\n",
       "       '3gram_cooccurence', '3gram_distinct', '3gram_nostpwrd_cooccurence',\n",
       "       '3gram_nostpwrd_distinct', 'spacy_similarity', 'q1_how', 'q2_how',\n",
       "       'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which',\n",
       "       'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where',\n",
       "       'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why',\n",
       "       'why_both', 'caps_count_q1', 'caps_count_q2', 'diff_caps',\n",
       "       'exactly_same'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "### A. Lightgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lgb_train import lgb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features on train matrix:  99\n",
      "Number of features on test matrix:  99\n"
     ]
    }
   ],
   "source": [
    "print('Number of features on train matrix: ',len(X_train.columns))\n",
    "print('Number of features on test matrix: ',len(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num_leaves in [130,140,150,160]:\n",
    "    for lr in [0.01,0.005,0.001]:\n",
    "        for lambda_l2 in [1,1.5,2]:\n",
    "            RANDOM_SEED = 2017\n",
    "            lgb_params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'boosting': 'gbdt',\n",
    "                'device': 'cpu',\n",
    "                'feature_fraction': 0.486,\n",
    "                'num_leaves': num_leaves,\n",
    "                'lambda_l2': lambda_l2,\n",
    "                'learning_rate': lr,\n",
    "                'num_boost_round': 5000,\n",
    "                'early_stopping_rounds': 50,\n",
    "                'max_depth': 25,\n",
    "                'min_data_in_leaf': 15,\n",
    "                'subsample': 1,\n",
    "                'colsample_bytree': 1,\n",
    "                'verbose': 1,\n",
    "                'bagging_fraction_seed': RANDOM_SEED,\n",
    "                'feature_fraction_seed': RANDOM_SEED,\n",
    "            }\n",
    "            print(lgb_params)\n",
    "            lgb_train(X_train, X_test, Y_train, lgb_params, log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Abderrahim best parameters\n",
    "RANDOM_SEED = 2017\n",
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'feature_fraction': 0.486,\n",
    "        'num_leaves': 140,\n",
    "        'lambda_l2': 2,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_boost_round': 5000,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'max_depth': 25,\n",
    "        'min_data_in_leaf': 15,\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'verbose': 1,\n",
    "        'bagging_fraction_seed': RANDOM_SEED,\n",
    "        'feature_fraction_seed': RANDOM_SEED,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lgb_params = {\n",
    "#    'learning_rate': 0.005, 'colsample_bytree': 1, 'boosting': 'gbdt', 'feature_fraction': 0.486, \n",
    "#    'metric': 'binary_logloss', 'min_data_in_leaf': 15, 'verbose': 1, 'subsample': 1, 'bagging_fraction_seed': 2017, \n",
    "#    'objective': 'binary', 'num_leaves': 130, 'max_depth': 25, 'early_stopping_rounds': 50, 'lambda_l2': 1.5, \n",
    "#    'feature_fraction_seed': 2017, 'device': 'cpu', 'num_boost_round': 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/lightgbm/engine.py:103: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 1672 rounds, training loss 0.046193, validation loss 0.133867\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 2: 1453 rounds, training loss 0.052660, validation loss 0.142120\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 3: 1546 rounds, training loss 0.049528, validation loss 0.144587\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 4: 1780 rounds, training loss 0.043762, validation loss 0.141389\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 5: 1300 rounds, training loss 0.056807, validation loss 0.150178\n",
      "Final CV val score: [0.13386710621237824, 0.14211991103984803, 0.14458690691737858, 0.14138932543157473, 0.15017844553049162]\n",
      "Final mean CV val score: 0.14242833902633426\n",
      "\n",
      "Make submission file...\n",
      "Submission file written !\n"
     ]
    }
   ],
   "source": [
    "feat_imp = lgb_train(X_train, X_test, Y_train, lgb_params, log_filepath, test_prediction=True, num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>shortest_path</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           column  importance\n",
       "64  shortest_path         828"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp[feat_imp['column']=='shortest_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train1, Y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "# UPDownSampling\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8 * len(pos_train))], neg_train))\n",
    "y_train = np.array(\n",
    "    [0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8 * len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "y_valid = np.array(\n",
    "    [0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    "# params['scale_pos_weight'] = 0.2\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.path.join(log_dir,'test','train','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
