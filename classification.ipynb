{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTEGRAD Challenge - Classification\n",
    "\n",
    "*Abderrahim AIT-AZZI, SÃ©bastien OHLEYER, Mickael SUTTON*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Performing grid search\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from log import _check_log_directory,_initialise_model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "log_dir = './log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove features\n",
    "features_train = pd.read_csv(os.path.join(data_dir,'train_features_glove.csv'), sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv(os.path.join(data_dir,'test_features_glove.csv'), sep=',', encoding='latin-1')\n",
    "features_train= features_train.drop(['question1', 'question2'], axis=1)\n",
    "features_test = features_test.drop(['id','qid1','qid2','question1', 'question2'], axis=1)\n",
    "data_train = pd.read_csv('./data/train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "Y_train=data_train[\"is_duplicate\"].values\n",
    "\n",
    "# Pagerank features\n",
    "pagerank_feats_train = pd.read_csv(os.path.join(data_dir,\"train_pagerank.csv\"), sep=',')\n",
    "pagerank_feats_test = pd.read_csv(os.path.join(data_dir,\"test_pagerank.csv\"), sep=',')\n",
    "\n",
    "# Question frequency\n",
    "train_question_freq = pd.read_csv(os.path.join(data_dir,'train_question_freq.csv'), sep=',', index_col=0)\n",
    "test_question_freq = pd.read_csv(os.path.join(data_dir,'test_question_freq.csv'), sep=',', index_col=0)\n",
    "\n",
    "# Intersection of questions\n",
    "train_question_inter= pd.read_csv(os.path.join(data_dir,'train_question_inter.csv'), sep=',', index_col=0)\n",
    "test_question_inter = pd.read_csv(os.path.join(data_dir,'test_question_inter.csv'), sep=',', index_col=0)\n",
    "\n",
    "# K-cores\n",
    "train_kcores = pd.read_csv(os.path.join(data_dir,'train_kcores.csv'), sep=',', index_col=0)\n",
    "test_kcores = pd.read_csv(os.path.join(data_dir,'test_kcores.csv'), sep=',', index_col=0)\n",
    "\n",
    "# question K-cores\n",
    "train_question_kcores = pd.read_csv(os.path.join(data_dir,'train_question_kcores.csv'), sep=',', index_col=0)\n",
    "test_question_kcores = pd.read_csv(os.path.join(data_dir,'test_question_kcores.csv'), sep=',', index_col=0)\n",
    "\n",
    "# TF-IDF\n",
    "train_tfidf = pd.read_csv(os.path.join(data_dir,'train_tfidf.csv'), sep=',', index_col=0)\n",
    "test_tfidf = pd.read_csv(os.path.join(data_dir,'test_tfidf.csv'), sep=',', index_col=0)\n",
    "\n",
    "# Graph features\n",
    "train_graph_feat = pd.read_csv(os.path.join(data_dir,'train_graph_feat.csv'), sep=',', index_col=0)\n",
    "test_graph_feat = pd.read_csv(os.path.join(data_dir,'test_graph_feat.csv'), sep=',', index_col=0)\n",
    "\n",
    "# Bigram feature\n",
    "train_bigram_feat = pd.read_csv(os.path.join(data_dir,'train_bigram_feat.csv'), sep=',', index_col=0)\n",
    "test_bigram_feat = pd.read_csv(os.path.join(data_dir,'test_bigram_feat.csv'), sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_question_kcores.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Pagerank features\n",
    "features_train[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_train[[\"q1_pr\",\"q2_pr\"]]\n",
    "features_test[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_test[[\"q1_pr\",\"q2_pr\"]]\n",
    "\n",
    "# Add question frequency features\n",
    "features_train[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=train_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "features_test[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=test_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "\n",
    "# Add intersection of questions features\n",
    "features_train['q1_q2_intersect']=train_question_inter['q1_q2_intersect']\n",
    "features_test['q1_q2_intersect']=test_question_inter['q1_q2_intersect']\n",
    "\n",
    "# Add K-cores\n",
    "features_train[[\"core1\",\"core2\",\"core3\"]] = train_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "features_test[[\"core1\",\"core2\",\"core3\"]] = test_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "\n",
    "# Add question K-cores features\n",
    "features_train[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "                'q1_q2_kcores_diff_normed']]=train_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "features_test[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "               'q1_q2_kcores_diff_normed']]=test_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "\n",
    "# Add TF-IDF features\n",
    "features_train[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=train_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "features_test[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=test_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "\n",
    "# Add graph features\n",
    "features_train[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']] = train_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']]\n",
    "features_test[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']] = test_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']]\n",
    "\n",
    "# Add bigram features\n",
    "features_train[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence', 'bigram_nostpwrd_coocurence']] = train_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence', 'bigram_nostpwrd_coocurence']]\n",
    "features_test[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence', 'bigram_nostpwrd_coocurence']] = test_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence', 'bigram_nostpwrd_coocurence']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= features_train\n",
    "X_test = features_test\n",
    "X_train=X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_train=X_train.fillna(value=0)\n",
    "X_test=X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test=X_test.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['is_duplicate', 'len_q1', 'len_q2', 'diff_len', 'len_char_q1',\n",
       "       'len_char_q2', 'len_word_q1', 'len_word_q2', 'common_words',\n",
       "       'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
       "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
       "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd',\n",
       "       'cosine_distance', 'cityblock_distance', 'jaccard_distance',\n",
       "       'canberra_distance', 'euclidean_distance', 'minkowski_distance',\n",
       "       'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec',\n",
       "       'kur_q2vec', 'q1_pr', 'q2_pr', 'q1_hash', 'q2_hash', 'q1_freq',\n",
       "       'q2_freq', 'q1_q2_intersect', 'core1', 'core2', 'core3', 'q1_kcores',\n",
       "       'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff',\n",
       "       'q1_q2_kcores_diff_normed', 'word_match', 'tfidf_wm', 'tfidf_wm_stops',\n",
       "       'jaccard', 'wc_diff', 'wc_ratio', 'wc_diff_unique', 'wc_ratio_unique',\n",
       "       'wc_diff_unq_stop', 'wc_ratio_unique_stop', 'same_start', 'char_diff',\n",
       "       'char_diff_unq_stop', 'total_unique_words', 'total_unq_words_stop',\n",
       "       'char_ratio', 'q1_neigh', 'q2_neigh', 'common_neigh', 'distinct_neigh',\n",
       "       'clique_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "### A. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model directory\n",
    "log_name = (datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "_check_log_directory(os.path.join(log_dir,log_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in X_train.columns if x not in ['is_duplicate']]\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "cv_folds=5\n",
    "early_stopping_rounds=50\n",
    "# modelfit(xgb1, X_train, predictors)\n",
    "alg=xgb1\n",
    "dtrain=X_train.copy()\n",
    "xgb_param = alg.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain['is_duplicate'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alg.booster().attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "    metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval=True)\n",
    "alg.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the algorithm on the data\n",
    "alg.fit(dtrain[predictors], dtrain['is_duplicate'],eval_metric='auc',verbose=True)\n",
    "\n",
    "#Predict training set:\n",
    "dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print (\"\\nModel Report\")\n",
    "print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['is_duplicate'].values, dtrain_predictions))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['is_duplicate'], dtrain_predprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#plt.ylabel('Feature Importance Score')\n",
    "#plt.show()\n",
    "print(feat_imp)\n",
    "#feat_imp.to_csv(os.path.join(log_dir,'feature_important.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dropping inefficient features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = features_train.drop(['is_duplicate','core1','core2','core3','q1_neigh','q2_neigh','len_word_q2','len_word_q1','same_start','wc_diff_unq_stop','wc_diff_unique','q1_freq','wc_diff','q1_kcores','q1_q2_kcores_diff','q1_q2_intersect','q1_q2_kcores_ratio','euclidean_distance','len_word_q2','q2_freq',\n",
    "#                              'norm_wmd','q1_q2_kcores_diff_normed','common_neigh','q2_kcores','diff_len','len_word_q1','common_words','clique_size','total_unique_words','total_unq_words_stop','wc_ratio_unique_stop','wc_ratio_unique'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = features_test.drop(['core1','core2','core3','q1_neigh','q2_neigh','len_word_q2','len_word_q1','same_start','wc_diff_unq_stop','wc_diff_unique','q1_freq','wc_diff','q1_kcores','q1_q2_kcores_diff','q1_q2_intersect','q1_q2_kcores_ratio','euclidean_distance','len_word_q2','q2_freq',\n",
    "#                              'norm_wmd','q1_q2_kcores_diff_normed','common_neigh','q2_kcores','diff_len','len_word_q1','common_words','clique_size','total_unique_words','total_unq_words_stop','wc_ratio_unique_stop','wc_ratio_unique'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = xgb1.predict_proba(X_test)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred1.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred1[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Cross val\n",
    "\n",
    "### B.1 light gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "NUM_FOLDS = 5\n",
    "RANDOM_SEED = 2017\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features on train matrix:  65\n",
      "Number of features on test matrix:  64\n"
     ]
    }
   ],
   "source": [
    "print('Number of features on train matrix: ',len(X_train.columns))\n",
    "print('Number of features on test matrix: ',len(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to make log directory at ./log/27-01-2018_18-22-28\n"
     ]
    }
   ],
   "source": [
    "#initialize model directory\n",
    "log_name = (datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "log_filepath = os.path.join(log_dir,log_name,'lighgb.csv')\n",
    "_check_log_directory(os.path.join(log_dir,log_name))\n",
    "_initialise_model_log(log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "64079\n",
      "(80100, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/lightgbm/engine.py:103: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 5000 rounds, training loss 0.000089, validation loss 0.000091\n",
      "\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "64080\n",
      "(80100, 65)\n",
      "Fold 2: 5000 rounds, training loss 0.000089, validation loss 0.000096\n",
      "\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "64080\n",
      "(80100, 65)\n",
      "Fold 3: 5000 rounds, training loss 0.000089, validation loss 0.000099\n",
      "\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "64080\n",
      "(80100, 65)\n",
      "Fold 4: 5000 rounds, training loss 0.000089, validation loss 0.000095\n",
      "\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "64081\n",
      "(80100, 65)\n",
      "Fold 5: 5000 rounds, training loss 0.000089, validation loss 0.000100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.zeros((len(X_test), NUM_FOLDS))\n",
    "cv_val_scores = []\n",
    "cv_train_scores = []\n",
    "\n",
    "X_train_values=X_train.values\n",
    "X_test_values=X_test.values\n",
    "for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_values, Y_train)):\n",
    "    print('Fitting fold {fold_num + 1} of {kfold.n_splits}')\n",
    "    \n",
    "    print(len(ix_train))\n",
    "    print(X_train_values.shape)\n",
    "    X_fold_train = X_train_values[ix_train,:]\n",
    "    X_fold_val = X_train_values[ix_val,:]\n",
    "\n",
    "    y_fold_train = Y_train[ix_train]\n",
    "    y_fold_val = Y_train[ix_val]\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'feature_fraction': 0.486,\n",
    "        'num_leaves': 170,\n",
    "        'lambda_l2': 50,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_boost_round': 5000,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'verbose': 1,\n",
    "        'bagging_fraction_seed': RANDOM_SEED,\n",
    "        'feature_fraction_seed': RANDOM_SEED,\n",
    "    }\n",
    "    \n",
    "    lgb_data_train = lgb.Dataset(X_fold_train, y_fold_train)\n",
    "    lgb_data_val = lgb.Dataset(X_fold_val, y_fold_val)    \n",
    "    evals_result = {}\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        lgb_data_train,\n",
    "        valid_sets=[lgb_data_train, lgb_data_val],\n",
    "        evals_result=evals_result,\n",
    "        num_boost_round=lgb_params['num_boost_round'],\n",
    "        early_stopping_rounds=lgb_params['early_stopping_rounds'],\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    \n",
    "    fold_train_scores = evals_result['training'][lgb_params['metric']]\n",
    "    fold_val_scores = evals_result['valid_1'][lgb_params['metric']]\n",
    "    \n",
    "    print('Fold {}: {} rounds, training loss {:.6f}, validation loss {:.6f}'.format(\n",
    "        fold_num + 1,\n",
    "        len(fold_train_scores),\n",
    "        fold_train_scores[-1],\n",
    "        fold_val_scores[-1],\n",
    "    ))\n",
    "    print()\n",
    "    cv_train_scores.append(fold_train_scores[-1])\n",
    "    cv_val_scores.append(fold_val_scores[-1])\n",
    "    y_test_pred[:, fold_num] = model.predict(X_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame({\n",
    "    'column': list(X_train.columns),\n",
    "    'importance': model.feature_importance(),\n",
    "}).sort_values(by='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CV score: [9.08968021240727e-05, 9.57143085722637e-05, 9.926108085904113e-05, 9.501631228094978e-05, 0.00010013330435253106]\n"
     ]
    }
   ],
   "source": [
    "print('Final CV score:', cv_val_scores)\n",
    "with open(log_filepath, 'a') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    data = [[datetime.now().strftime('%d-%m-%Y_%H-%M-%S'), 'lightgb', \n",
    "             cv_val_scores, np.mean(cv_val_scores), \n",
    "             cv_train_scores, np.mean(cv_train_scores),\n",
    "             lgb_params, feat_imp.to_dict()]]\n",
    "    a.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.mean(y_test_pred, axis=1)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_test.shape[0]):\n",
    "        f.write(str(i)+','+str(y_test[i])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train1, Y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "# UPDownSampling\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8 * len(pos_train))], neg_train))\n",
    "y_train = np.array(\n",
    "    [0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8 * len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "y_valid = np.array(\n",
    "    [0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    "# params['scale_pos_weight'] = 0.2\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(log_dir,'test','train','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
