{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTEGRAD Challenge - Classification\n",
    "\n",
    "*Abderrahim AIT-AZZI, SÃ©bastien OHLEYER, Mickael SUTTON*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Performing grid search\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from log import _check_log_directory,_initialise_model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data/\"\n",
    "log_dir = './log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove features\n",
    "features_train = pd.read_csv(data_dir+'train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv(data_dir+'test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_train= features_train.drop(['question1', 'question2'], axis=1)\n",
    "features_test = features_test.drop(['id','qid1','qid2','question1', 'question2'], axis=1)\n",
    "data_train = pd.read_csv(data_dir+'train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "\n",
    "# Pagerank features\n",
    "pagerank_feats_train = pd.read_csv(data_dir+\"train_pagerank.csv\", sep=',')\n",
    "pagerank_feats_test = pd.read_csv(data_dir+\"test_pagerank.csv\", sep=',')\n",
    "\n",
    "# Question frequency\n",
    "train_question_freq = pd.read_csv(data_dir+'train_question_freq.csv', sep=',', index_col=0)\n",
    "test_question_freq = pd.read_csv(data_dir+'test_question_freq.csv', sep=',', index_col=0)\n",
    "\n",
    "# Intersection of questions\n",
    "train_question_inter= pd.read_csv(data_dir+'train_question_inter.csv', sep=',', index_col=0)\n",
    "test_question_inter = pd.read_csv(data_dir+'test_question_inter.csv', sep=',', index_col=0)\n",
    "\n",
    "# K-cores\n",
    "train_kcores = pd.read_csv(data_dir+'train_kcores.csv', sep=',', index_col=0)\n",
    "test_kcores = pd.read_csv(data_dir+'test_kcores.csv', sep=',', index_col=0)\n",
    "\n",
    "# question K-cores\n",
    "train_question_kcores = pd.read_csv(data_dir+'train_question_kcores.csv', sep=',', index_col=0)\n",
    "test_question_kcores = pd.read_csv(data_dir+'test_question_kcores.csv', sep=',', index_col=0)\n",
    "\n",
    "# TF-IDF\n",
    "train_tfidf = pd.read_csv(data_dir+'train_tfidf.csv', sep=',', index_col=0)\n",
    "test_tfidf = pd.read_csv(data_dir+'test_tfidf.csv', sep=',', index_col=0)\n",
    "\n",
    "# Graph features\n",
    "train_graph_feat = pd.read_csv(data_dir+'train_graph_feat.csv', sep=',', index_col=0)\n",
    "test_graph_feat = pd.read_csv(data_dir+'test_graph_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# Bigram feature\n",
    "train_bigram_feat = pd.read_csv(data_dir+'train_2gram_feat.csv', sep=',', index_col=0)\n",
    "test_bigram_feat = pd.read_csv(data_dir+'test_2gram_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# 3gram feature\n",
    "train_3gram_feat = pd.read_csv(data_dir+'train_3gram_feat.csv', sep=',', index_col=0)\n",
    "test_3gram_feat = pd.read_csv(data_dir+'test_3gram_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# spaCy feature\n",
    "train_spacy_feat = pd.read_csv(data_dir+'train_spacy_features.csv', sep=',', index_col=0)\n",
    "test_spacy_feat = pd.read_csv(data_dir+'test_spacy_features.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pagerank features\n",
    "features_train[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_train[[\"q1_pr\",\"q2_pr\"]]\n",
    "features_test[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_test[[\"q1_pr\",\"q2_pr\"]]\n",
    "\n",
    "# Add question frequency features\n",
    "features_train[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=train_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "features_test[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=test_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "\n",
    "# Add intersection of questions features\n",
    "features_train['q1_q2_intersect']=train_question_inter['q1_q2_intersect']\n",
    "features_test['q1_q2_intersect']=test_question_inter['q1_q2_intersect']\n",
    "\n",
    "# Add K-cores\n",
    "features_train[[\"core1\",\"core2\",\"core3\"]] = train_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "features_test[[\"core1\",\"core2\",\"core3\"]] = test_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "\n",
    "# Add question K-cores features\n",
    "features_train[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "                'q1_q2_kcores_diff_normed']]=train_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "features_test[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "               'q1_q2_kcores_diff_normed']]=test_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "\n",
    "# Add TF-IDF features\n",
    "features_train[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=train_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "features_test[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=test_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "\n",
    "# Add graph features\n",
    "features_train[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']] = train_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']]\n",
    "features_test[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']] = test_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size']]\n",
    "\n",
    "# Add bigram features\n",
    "features_train[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']] = train_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "features_test[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']] = test_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "\n",
    "# Add 3gram features\n",
    "features_train[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']] = train_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "features_test[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']] = test_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "\n",
    "# Add spaCy features\n",
    "features_train[['spacy_similarity']] = train_spacy_feat[['spacy_similarity']]\n",
    "features_test[['spacy_similarity']] = test_spacy_feat[['spacy_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= features_train.drop(['is_duplicate'],axis=1)\n",
    "X_test = features_test\n",
    "X_train=X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_train=X_train.fillna(value=0)\n",
    "X_test=X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test=X_test.fillna(value=0)\n",
    "Y_train = data_train[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['len_q1', 'len_q2', 'diff_len', 'len_char_q1', 'len_char_q2',\n",
       "       'len_word_q1', 'len_word_q2', 'common_words', 'fuzz_qratio',\n",
       "       'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
       "       'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio',\n",
       "       'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cosine_distance',\n",
       "       'cityblock_distance', 'jaccard_distance', 'canberra_distance',\n",
       "       'euclidean_distance', 'minkowski_distance', 'braycurtis_distance',\n",
       "       'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec', 'q1_pr', 'q2_pr',\n",
       "       'q1_hash', 'q2_hash', 'q1_freq', 'q2_freq', 'q1_q2_intersect', 'core1',\n",
       "       'core2', 'core3', 'q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio',\n",
       "       'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed', 'word_match',\n",
       "       'tfidf_wm', 'tfidf_wm_stops', 'jaccard', 'wc_diff', 'wc_ratio',\n",
       "       'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unq_stop',\n",
       "       'wc_ratio_unique_stop', 'same_start', 'char_diff', 'char_diff_unq_stop',\n",
       "       'total_unique_words', 'total_unq_words_stop', 'char_ratio', 'q1_neigh',\n",
       "       'q2_neigh', 'common_neigh', 'distinct_neigh', 'clique_size',\n",
       "       'bigram_coocurence', 'bigram_distinct', 'bigram_nostpwrd_coocurence',\n",
       "       'bigram_nostpwrd_distinct', '3gram_cooccurence', '3gram_distinct',\n",
       "       '3gram_nostpwrd_cooccurence', '3gram_nostpwrd_distinct',\n",
       "       'spacy_similarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "### A. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model directory\n",
    "log_name = (datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "_check_log_directory(os.path.join(log_dir,log_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in X_train.columns if x not in ['is_duplicate']]\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "cv_folds=5\n",
    "early_stopping_rounds=50\n",
    "# modelfit(xgb1, X_train, predictors)\n",
    "alg=xgb1\n",
    "dtrain=X_train.copy()\n",
    "xgb_param = alg.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain['is_duplicate'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alg.booster().attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "    metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval=True)\n",
    "alg.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the algorithm on the data\n",
    "alg.fit(dtrain[predictors], dtrain['is_duplicate'],eval_metric='auc',verbose=True)\n",
    "\n",
    "#Predict training set:\n",
    "dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print (\"\\nModel Report\")\n",
    "print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['is_duplicate'].values, dtrain_predictions))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['is_duplicate'], dtrain_predprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#plt.ylabel('Feature Importance Score')\n",
    "#plt.show()\n",
    "print(feat_imp)\n",
    "#feat_imp.to_csv(os.path.join(log_dir,'feature_important.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dropping inefficient features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = features_train.drop(['is_duplicate','core1','core2','core3','q1_neigh','q2_neigh','len_word_q2','len_word_q1','same_start','wc_diff_unq_stop','wc_diff_unique','q1_freq','wc_diff','q1_kcores','q1_q2_kcores_diff','q1_q2_intersect','q1_q2_kcores_ratio','euclidean_distance','len_word_q2','q2_freq',\n",
    "#                              'norm_wmd','q1_q2_kcores_diff_normed','common_neigh','q2_kcores','diff_len','len_word_q1','common_words','clique_size','total_unique_words','total_unq_words_stop','wc_ratio_unique_stop','wc_ratio_unique'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = features_test.drop(['core1','core2','core3','q1_neigh','q2_neigh','len_word_q2','len_word_q1','same_start','wc_diff_unq_stop','wc_diff_unique','q1_freq','wc_diff','q1_kcores','q1_q2_kcores_diff','q1_q2_intersect','q1_q2_kcores_ratio','euclidean_distance','len_word_q2','q2_freq',\n",
    "#                              'norm_wmd','q1_q2_kcores_diff_normed','common_neigh','q2_kcores','diff_len','len_word_q1','common_words','clique_size','total_unique_words','total_unq_words_stop','wc_ratio_unique_stop','wc_ratio_unique'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = xgb1.predict_proba(X_test)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred1.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred1[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Cross val\n",
    "\n",
    "### B.1 light gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lgb_train import lgb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features on train matrix:  73\n",
      "Number of features on test matrix:  73\n"
     ]
    }
   ],
   "source": [
    "print('Number of features on train matrix: ',len(X_train.columns))\n",
    "print('Number of features on test matrix: ',len(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to make log directory at ./log/28-01-2018_23-13-22\n"
     ]
    }
   ],
   "source": [
    "#initialize model directory\n",
    "log_name = (datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "log_filepath = os.path.join(log_dir,log_name,'lighgb.csv')\n",
    "_check_log_directory(os.path.join(log_dir,log_name))\n",
    "_initialise_model_log(log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num_leaves in [130,140,150,160]:\n",
    "    for lr in [0.01,0.005,0.001]:\n",
    "        for lambda_l2 in [1,1.5,2]:\n",
    "            RANDOM_SEED = 2017\n",
    "            lgb_params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'boosting': 'gbdt',\n",
    "                'device': 'cpu',\n",
    "                'feature_fraction': 0.486,\n",
    "                'num_leaves': num_leaves,\n",
    "                'lambda_l2': lambda_l2,\n",
    "                'learning_rate': lr,\n",
    "                'num_boost_round': 5000,\n",
    "                'early_stopping_rounds': 50,\n",
    "                'max_depth': 25,\n",
    "                'min_data_in_leaf': 15,\n",
    "                'subsample': 1,\n",
    "                'colsample_bytree': 1,\n",
    "                'verbose': 1,\n",
    "                'bagging_fraction_seed': RANDOM_SEED,\n",
    "                'feature_fraction_seed': RANDOM_SEED,\n",
    "            }\n",
    "            print(lgb_params)\n",
    "            lgb_train(X_train, X_test, Y_train, lgb_params, log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Abderrahim best parameters\n",
    "RANDOM_SEED = 2017\n",
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'feature_fraction': 0.486,\n",
    "        'num_leaves': 140,\n",
    "        'lambda_l2': 2,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_boost_round': 5000,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'max_depth': 25,\n",
    "         'min_data_in_leaf': 15,\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'verbose': 1,\n",
    "        'bagging_fraction_seed': RANDOM_SEED,\n",
    "        'feature_fraction_seed': RANDOM_SEED,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'learning_rate': 0.005, 'colsample_bytree': 1, 'boosting': 'gbdt', 'feature_fraction': 0.486, \n",
    "    'metric': 'binary_logloss', 'min_data_in_leaf': 15, 'verbose': 1, 'subsample': 1, 'bagging_fraction_seed': 2017, \n",
    "    'objective': 'binary', 'num_leaves': 130, 'max_depth': 25, 'early_stopping_rounds': 50, 'lambda_l2': 1.5, \n",
    "    'feature_fraction_seed': 2017, 'device': 'cpu', 'num_boost_round': 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/sebastienohleyer/anaconda3/envs/nlp/lib/python3.6/site-packages/lightgbm/engine.py:103: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 1638 rounds, training loss 0.048931, validation loss 0.135226\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 2: 1505 rounds, training loss 0.052507, validation loss 0.144312\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 3: 1508 rounds, training loss 0.052297, validation loss 0.145581\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 4: 1374 rounds, training loss 0.056978, validation loss 0.144075\n",
      "Fitting fold {fold_num + 1} of {kfold.n_splits}\n",
      "Fold 5: 1249 rounds, training loss 0.060178, validation loss 0.151146\n",
      "Final CV val score: [0.1352257148669988, 0.1443116008223044, 0.1455808199670401, 0.14407515129570092, 0.15114601945582062]\n",
      "Final mean CV val score: 0.14406786128157295\n",
      "\n",
      "Make submission file...\n",
      "Submission file written !\n"
     ]
    }
   ],
   "source": [
    "lgb_train(X_train, X_test, Y_train, lgb_params, log_filepath, test_prediction=True, num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train1, Y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "# UPDownSampling\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8 * len(pos_train))], neg_train))\n",
    "y_train = np.array(\n",
    "    [0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8 * len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "y_valid = np.array(\n",
    "    [0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    "# params['scale_pos_weight'] = 0.2\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(log_dir,'test','train','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
