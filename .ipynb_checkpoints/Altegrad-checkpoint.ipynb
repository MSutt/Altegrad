{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMD Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'C:/Users/Amine/Desktop/MVA2017/Object_recognition/Project/project/Project/KCCA/glove.42B.300d.txt.word2vec'\n",
    "word_embedding_model_glove = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Amine/Desktop/MVA2017/ALTEGRAD/TP3/for moodle/code/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features for train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some useful features:\n",
    "\n",
    "    -Using the fuzz package to compute some ratio of string similarity between question 1 et question 2.\n",
    "    -The length of questions\n",
    "    -Word embedding of the questions and compute different distances: WMD, cosine, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "data_train = data_train.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding of the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=word_embedding_model_glove\n",
    "norm_model = word_embedding_model_glove\n",
    "norm_model.init_sims(replace=True)\n",
    "\n",
    "data_train['wmd'] = data_train.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data_train.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data_train.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data_train.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data_train.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_train['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data_train['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_train['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(question1_vectors, open('data/q1_glove.pkl', 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open('data/q2_glove.pkl', 'wb'), -1)\n",
    "\n",
    "data_train.to_csv('data/train_features_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generete features for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['len_q1'] = data_test.question1.apply(lambda x: len(str(x)))\n",
    "data_test['len_q2'] = data_test.question2.apply(lambda x: len(str(x)))\n",
    "data_test['diff_len'] = data_test.len_q1 - data_test.len_q2\n",
    "data_test['len_char_q1'] = data_test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_test['len_char_q2'] = data_test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_test['len_word_q1'] = data_test.question1.apply(lambda x: len(str(x).split()))\n",
    "data_test['len_word_q2'] = data_test.question2.apply(lambda x: len(str(x).split()))\n",
    "data_test['common_words'] = data_test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_test['fuzz_qratio'] = data_test.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_WRatio'] = data_test.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_ratio'] = data_test.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_token_set_ratio'] = data_test.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_token_sort_ratio'] = data_test.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_token_set_ratio'] = data_test.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_token_sort_ratio'] = data_test.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_model = model\n",
    "norm_model.init_sims(replace=True)\n",
    "data_test['wmd'] = data_test.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data_test.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data_test.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data_test.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data_test.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_test['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data_test['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_test['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(question1_vectors, open('data/q1_glove_test.pkl', 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open('data/q2_glove_test.pkl', 'wb'), -1)\n",
    "data_test.to_csv('data/test_features_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = pd.read_csv('data/train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv('data/test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "data_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "Y_train=data_train[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/zfturbo/pagerank-on-quora-feature-file-generator/code\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import gc \n",
    "\n",
    "df_train = pd.read_csv('data/train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "df_test = pd.read_csv('data/test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "# Generating a graph of Questions and their neighbors\n",
    "def generate_qid_graph_table(row):\n",
    "    hash_key1 = hashlib.md5(row[\"question1\"].encode('utf-8')).hexdigest()\n",
    "    hash_key2 = hashlib.md5(row[\"question2\"].encode('utf-8')).hexdigest()\n",
    "\n",
    "    qid_graph.setdefault(hash_key1, []).append(hash_key2)\n",
    "    qid_graph.setdefault(hash_key2, []).append(hash_key1)\n",
    "\n",
    "\n",
    "qid_graph = {}\n",
    "print('Apply to train...')\n",
    "df_train.apply(generate_qid_graph_table, axis=1)\n",
    "print('Apply to test...')\n",
    "df_test.apply(generate_qid_graph_table, axis=1)\n",
    "\n",
    "\n",
    "def pagerank():\n",
    "    MAX_ITER = 20\n",
    "    d = 0.85\n",
    "\n",
    "    # Initializing -- every node gets a uniform value!\n",
    "    pagerank_dict = {i: 1 / len(qid_graph) for i in qid_graph}\n",
    "    num_nodes = len(pagerank_dict)\n",
    "\n",
    "    for iter in range(0, MAX_ITER):\n",
    "\n",
    "        for node in qid_graph:\n",
    "            local_pr = 0\n",
    "\n",
    "            for neighbor in qid_graph[node]:\n",
    "                local_pr += pagerank_dict[neighbor] / len(qid_graph[neighbor])\n",
    "\n",
    "            pagerank_dict[node] = (1 - d) / num_nodes + d * local_pr\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "print('Main PR generator...')\n",
    "pagerank_dict = pagerank()\n",
    "\n",
    "def get_pagerank_value(row):\n",
    "    q1 = hashlib.md5(row[\"question1\"].encode('utf-8')).hexdigest()\n",
    "    q2 = hashlib.md5(row[\"question2\"].encode('utf-8')).hexdigest()\n",
    "    s = pd.Series({\n",
    "        \"q1_pr\": pagerank_dict[q1],\n",
    "        \"q2_pr\": pagerank_dict[q2]\n",
    "    })\n",
    "    return s\n",
    "\n",
    "print('Apply to train...')\n",
    "pagerank_feats_train = df_train.apply(get_pagerank_value, axis=1)\n",
    "print('Writing train...')\n",
    "pagerank_feats_train.to_csv(\"data/pagerank_train.csv\", index=False)\n",
    "del df_train\n",
    "gc.collect()\n",
    "print('Apply to test...')\n",
    "pagerank_feats_test = df_test.apply(get_pagerank_value, axis=1)\n",
    "print('Writing test...')\n",
    "pagerank_feats_test.to_csv(\"data/pagerank_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features of page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test[\"q1_pr\"]=pagerank_feats_test[\"q1_pr\"]\n",
    "features_test[\"q2_pr\"]=pagerank_feats_test[\"q2_pr\"]\n",
    "features_train[\"q1_pr\"]=pagerank_feats_train[\"q1_pr\"]\n",
    "features_train[\"q2_pr\"]=pagerank_feats_train[\"q2_pr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has the purpose to compute the fraquency of the questions with the idea that more frequenct questions are more likely to be duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code from https://www.kaggle.com/jturkewitz/magic-features-0-03-gain/comments\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "train_orig =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "test_orig =  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "tic0=timeit.default_timer()\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df1_test = test_orig[['question1']].copy()\n",
    "df2_test = test_orig[['question2']].copy()\n",
    "\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "train_cp = train_orig.copy()\n",
    "test_cp = test_orig.copy()\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "# print(comb['q1_hash'])\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "corr_mat = train_comb.corr()\n",
    "train_comb.to_csv('./data/train_magic.csv', columns=['q1_freq', 'q2_freq'])\n",
    "test_comb.to_csv('./data/test_magic.csv', columns=['q1_freq', 'q2_freq'])\n",
    "print(corr_mat)\n",
    "print(test_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "train_orig =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "test_orig =  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "ques = pd.concat([train_orig[['question1', 'question2']], test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "\n",
    "\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "    q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "    q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "train_orig['q1_q2_intersect'] = train_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_orig['q1_q2_intersect'] = test_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "\n",
    "train_feat = train_orig[['q1_q2_intersect']]\n",
    "test_feat = test_orig[['q1_q2_intersect']]\n",
    "\n",
    "train_feat.to_csv('./data/train_magic_v2.csv')\n",
    "test_feat.to_csv('./data/test_magic_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = pd.read_csv('data/train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv('data/test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_train= features_train.drop(['question1', 'question2'], axis=1)\n",
    "features_test = features_test.drop(['id','qid1','qid2','question1', 'question2'], axis=1)\n",
    "data_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "Y_train=data_train[\"is_duplicate\"].values\n",
    "features_test[\"q1_pr\"]=pagerank_feats_test[\"q1_pr\"]\n",
    "features_test[\"q2_pr\"]=pagerank_feats_test[\"q2_pr\"]\n",
    "features_train[\"q1_pr\"]=pagerank_feats_train[\"q1_pr\"]\n",
    "features_train[\"q2_pr\"]=pagerank_feats_train[\"q2_pr\"]\n",
    "features_test[\"q1_hash\"]=test_comb[\"q1_hash\"]\n",
    "features_test[\"q2_hash\"]=test_comb[\"q2_hash\"]\n",
    "features_test[\"q1_freq\"]=test_comb[\"q1_freq\"]\n",
    "features_test[\"q2_freq\"]=test_comb[\"q2_freq\"]\n",
    "features_train[\"q1_hash\"]=train_comb[\"q1_hash\"]\n",
    "features_train[\"q2_hash\"]=train_comb[\"q1_hash\"]\n",
    "features_train[\"q1_freq\"]=train_comb[\"q1_freq\"]\n",
    "features_train[\"q2_freq\"]=train_comb[\"q2_freq\"]\n",
    "features_train['q1_q2_intersect']=train_feat['q1_q2_intersect']\n",
    "features_test['q1_q2_intersect']=test_feat['q1_q2_intersect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in X_train.columns if x not in ['is_duplicate']]\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "cv_folds=5\n",
    "early_stopping_rounds=50\n",
    "# modelfit(xgb1, X_train, predictors)\n",
    "alg=xgb1\n",
    "dtrain=X_train.copy()\n",
    "xgb_param = alg.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain['is_duplicate'].values)\n",
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "    metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "#Fit the algorithm on the data\n",
    "alg.fit(dtrain[predictors], dtrain['is_duplicate'],eval_metric='auc')\n",
    "\n",
    "#Predict training set:\n",
    "dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "\n",
    "#Print model report:\n",
    "print (\"\\nModel Report\")\n",
    "print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['is_duplicate'].values, dtrain_predictions))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['is_duplicate'], dtrain_predprob))\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= features_train.drop(['question1','question2','is_duplicate','cosine_distance','jaccard_distance','euclidean_distance','norm_wmd','fuzz_WRatio','len_word_q2','len_word_q1','minkowski_distance','braycurtis_distance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test= features_test.drop(['qid1', 'id', 'qid2','question1','question2','cosine_distance','jaccard_distance','euclidean_distance','norm_wmd','fuzz_WRatio','len_word_q2','len_word_q1','minkowski_distance','braycurtis_distance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1 = xgb1.predict_proba(X_test1)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred1.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred1[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### light gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "NUM_FOLDS = 5\n",
    "RANDOM_SEED = 2017\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.zeros((len(X_test1), NUM_FOLDS))\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=X_train1.values\n",
    "X_test=X_test1.values\n",
    "for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train, Y_train)):\n",
    "    print('Fitting fold {fold_num + 1} of {kfold.n_splits}')\n",
    "    \n",
    "    print(len(ix_train))\n",
    "    print(X_train.shape)\n",
    "    X_fold_train = X_train[ix_train,:]\n",
    "    X_fold_val = X_train[ix_val,:]\n",
    "\n",
    "    y_fold_train = Y_train[ix_train]\n",
    "    y_fold_val = Y_train[ix_val]\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'feature_fraction': 0.486,\n",
    "        'num_leaves': 158,\n",
    "        'lambda_l2': 50,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_boost_round': 5000,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'verbose': 1,\n",
    "        'bagging_fraction_seed': RANDOM_SEED,\n",
    "        'feature_fraction_seed': RANDOM_SEED,\n",
    "    }\n",
    "    \n",
    "    lgb_data_train = lgb.Dataset(X_fold_train, y_fold_train)\n",
    "    lgb_data_val = lgb.Dataset(X_fold_val, y_fold_val)    \n",
    "    evals_result = {}\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        lgb_data_train,\n",
    "        valid_sets=[lgb_data_train, lgb_data_val],\n",
    "        evals_result=evals_result,\n",
    "        num_boost_round=lgb_params['num_boost_round'],\n",
    "        early_stopping_rounds=lgb_params['early_stopping_rounds'],\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    \n",
    "    fold_train_scores = evals_result['training'][lgb_params['metric']]\n",
    "    fold_val_scores = evals_result['valid_1'][lgb_params['metric']]\n",
    "    \n",
    "    print('Fold {}: {} rounds, training loss {:.6f}, validation loss {:.6f}'.format(\n",
    "        fold_num + 1,\n",
    "        len(fold_train_scores),\n",
    "        fold_train_scores[-1],\n",
    "        fold_val_scores[-1],\n",
    "    ))\n",
    "    print()\n",
    "    \n",
    "    cv_scores.append(fold_val_scores[-1])\n",
    "    y_test_pred[:, fold_num] = model.predict(X_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'column': list(X_train.columns),\n",
    "    'importance': model.feature_importance(),\n",
    "}).sort_values(by='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Final CV score:', final_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.mean(y_test_pred, axis=1)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_test.shape[0]):\n",
    "        f.write(str(i)+','+str(y_test[i])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=18)\n",
    "X_test=X_test.values\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "X=X_train\n",
    "y=Y_train\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.75\n",
    "params['base_score'] = 0.2\n",
    "\n",
    "params['colsample_bytree'] = 1\n",
    "params['colsample_bylevel'] = 1\n",
    "params['n_jobs'] = -1\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_tr, y_val = y[train_index], y[test_index]\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    d_valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=50, verbose_eval=200)\n",
    "    val_loss = log_loss(y_val, bst.predict(d_valid))\n",
    "    print(val_loss)\n",
    "    \n",
    "    p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
