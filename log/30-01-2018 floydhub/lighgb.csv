DATETIME,Model,CV val scores,Mean CV val scores,CV train scores,Mean CV train scores,Parameters,Features importance
30-01-2018_15-38-49,lightgb,"[0.12197001426807404, 0.11901156285672809, 0.12911962047479866, 0.11795761885510204, 0.12306009446380653]",0.122223782184,"[0.041816511580016619, 0.033847090772306861, 0.045769157551298449, 0.041895200071452011, 0.041819955500215084]",0.0410295830951,"{'num_leaves': 120, 'num_boost_round': 5000, 'lambda_l2': 1.8, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2053, 1: 1984, 2: 1288, 3: 1523, 4: 1292, 5: 769, 6: 611, 7: 662, 8: 2282, 9: 1485, 10: 2722, 11: 0, 12: 2553, 13: 2197, 14: 3015, 15: 3450, 16: 1730, 17: 1702, 18: 1977, 19: 182, 20: 3080, 21: 1282, 22: 2006, 23: 2464, 24: 2665, 25: 2627, 26: 2797, 27: 2772, 28: 4305, 29: 4384, 30: 3624, 31: 3455, 32: 786, 33: 1075, 34: 583, 35: 319, 36: 441, 37: 505, 38: 644, 39: 805, 40: 985, 41: 870, 42: 792, 43: 2907, 44: 3820, 45: 3690, 46: 2171, 47: 736, 48: 1266, 49: 638, 50: 1343, 51: 475, 52: 1383, 53: 328, 54: 1408, 55: 1596, 56: 730, 57: 996, 58: 2020, 59: 353, 60: 459, 61: 601, 62: 1934, 63: 914, 64: 401, 65: 1448, 66: 1679, 67: 1025, 68: 2135, 69: 1344, 70: 1681, 71: 1060, 72: 1341, 73: 3400, 74: 1972, 75: 220, 76: 217, 77: 173, 78: 239, 79: 286, 80: 198, 81: 66, 82: 111, 83: 5, 84: 35, 85: 33, 86: 98, 87: 109, 88: 18, 89: 5, 90: 39, 91: 90, 92: 15, 93: 210, 94: 163, 95: 361, 96: 0, 97: 0, 98: 0, 99: 0, 100: 389, 101: 399, 102: 211, 103: 189, 104: 1232, 105: 1152, 106: 748, 107: 563, 108: 1170, 109: 903, 110: 515, 111: 480, 112: 683, 113: 773, 114: 445, 115: 340, 116: 600, 117: 466, 118: 630, 119: 620, 120: 70, 121: 36, 122: 1093, 123: 797, 124: 900, 125: 1062, 126: 452, 127: 647, 128: 211, 129: 117, 130: 697, 131: 689, 132: 71, 133: 91, 134: 1121, 135: 1165, 136: 705, 137: 854, 138: 187, 139: 219, 140: 991, 141: 841, 142: 766, 143: 888, 144: 996, 145: 1048, 146: 629, 147: 588, 148: 1161, 149: 1045, 150: 475, 151: 517, 152: 457, 153: 454, 154: 326, 155: 314, 156: 768, 157: 743}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_15-48-19,lightgb,"[0.12194448495814457, 0.119113117190646, 0.12889695955227373, 0.11791186890045748, 0.12272627533869103]",0.122118541188,"[0.041085512361846946, 0.038824379734161342, 0.04167019884145954, 0.037889436497246534, 0.040855989243825164]",0.0400651033357,"{'num_leaves': 120, 'num_boost_round': 5000, 'lambda_l2': 1.9, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2165, 1: 1975, 2: 1339, 3: 1582, 4: 1355, 5: 817, 6: 631, 7: 674, 8: 2333, 9: 1519, 10: 2907, 11: 0, 12: 2619, 13: 2309, 14: 3072, 15: 3491, 16: 1796, 17: 1733, 18: 2065, 19: 184, 20: 3190, 21: 1364, 22: 2094, 23: 2506, 24: 2772, 25: 2799, 26: 2947, 27: 2832, 28: 4436, 29: 4508, 30: 3909, 31: 3605, 32: 840, 33: 1092, 34: 606, 35: 328, 36: 446, 37: 525, 38: 635, 39: 829, 40: 1028, 41: 920, 42: 807, 43: 2998, 44: 3849, 45: 3853, 46: 2272, 47: 768, 48: 1437, 49: 664, 50: 1365, 51: 484, 52: 1437, 53: 333, 54: 1461, 55: 1620, 56: 743, 57: 1041, 58: 2213, 59: 340, 60: 441, 61: 616, 62: 1981, 63: 923, 64: 428, 65: 1515, 66: 1774, 67: 1029, 68: 2236, 69: 1414, 70: 1754, 71: 1090, 72: 1323, 73: 3520, 74: 2032, 75: 240, 76: 213, 77: 176, 78: 243, 79: 281, 80: 200, 81: 61, 82: 115, 83: 4, 84: 40, 85: 30, 86: 99, 87: 100, 88: 20, 89: 13, 90: 30, 91: 103, 92: 23, 93: 210, 94: 160, 95: 370, 96: 0, 97: 0, 98: 0, 99: 0, 100: 411, 101: 395, 102: 193, 103: 215, 104: 1258, 105: 1233, 106: 813, 107: 620, 108: 1198, 109: 982, 110: 530, 111: 487, 112: 731, 113: 754, 114: 446, 115: 355, 116: 626, 117: 509, 118: 636, 119: 661, 120: 74, 121: 35, 122: 1154, 123: 810, 124: 918, 125: 1060, 126: 462, 127: 682, 128: 235, 129: 118, 130: 737, 131: 752, 132: 67, 133: 102, 134: 1230, 135: 1209, 136: 789, 137: 901, 138: 179, 139: 200, 140: 999, 141: 886, 142: 813, 143: 867, 144: 998, 145: 1045, 146: 610, 147: 644, 148: 1198, 149: 1098, 150: 480, 151: 497, 152: 480, 153: 499, 154: 358, 155: 314, 156: 859, 157: 775}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_15-58-00,lightgb,"[0.12180451230838255, 0.1189718931671091, 0.12893808889450972, 0.11797223120249184, 0.12268291327346384]",0.122073927769,"[0.040554739251609011, 0.035809190361272357, 0.045915931856374714, 0.040169131873946218, 0.037301737112015849]",0.039950146091,"{'num_leaves': 120, 'num_boost_round': 5000, 'lambda_l2': 2, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2403, 1: 2126, 2: 1515, 3: 1761, 4: 1545, 5: 862, 6: 692, 7: 727, 8: 2591, 9: 1681, 10: 2989, 11: 0, 12: 2937, 13: 2509, 14: 3311, 15: 3678, 16: 1932, 17: 1905, 18: 2318, 19: 178, 20: 3424, 21: 1533, 22: 2228, 23: 2641, 24: 3074, 25: 3049, 26: 3268, 27: 3132, 28: 4750, 29: 4951, 30: 4357, 31: 3964, 32: 808, 33: 1145, 34: 634, 35: 314, 36: 448, 37: 555, 38: 700, 39: 884, 40: 1080, 41: 951, 42: 865, 43: 3186, 44: 4278, 45: 4082, 46: 2412, 47: 823, 48: 1567, 49: 702, 50: 1630, 51: 520, 52: 1566, 53: 380, 54: 1764, 55: 1835, 56: 807, 57: 1098, 58: 2388, 59: 414, 60: 470, 61: 621, 62: 2136, 63: 931, 64: 440, 65: 1636, 66: 1971, 67: 1166, 68: 2398, 69: 1457, 70: 1953, 71: 1212, 72: 1438, 73: 4040, 74: 2112, 75: 243, 76: 221, 77: 188, 78: 257, 79: 296, 80: 225, 81: 78, 82: 112, 83: 7, 84: 35, 85: 24, 86: 99, 87: 113, 88: 20, 89: 13, 90: 34, 91: 113, 92: 19, 93: 222, 94: 179, 95: 398, 96: 0, 97: 0, 98: 0, 99: 0, 100: 445, 101: 430, 102: 230, 103: 218, 104: 1364, 105: 1357, 106: 863, 107: 699, 108: 1378, 109: 1068, 110: 602, 111: 545, 112: 790, 113: 914, 114: 502, 115: 388, 116: 688, 117: 542, 118: 746, 119: 779, 120: 81, 121: 33, 122: 1227, 123: 937, 124: 1136, 125: 1159, 126: 534, 127: 733, 128: 258, 129: 131, 130: 769, 131: 864, 132: 66, 133: 104, 134: 1302, 135: 1275, 136: 887, 137: 996, 138: 205, 139: 203, 140: 1153, 141: 1011, 142: 876, 143: 1030, 144: 1081, 145: 1229, 146: 719, 147: 660, 148: 1304, 149: 1215, 150: 528, 151: 579, 152: 539, 153: 562, 154: 396, 155: 382, 156: 910, 157: 798}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_16-07-39,lightgb,"[0.12162033414755269, 0.11887288650203089, 0.12909627140639751, 0.11817935553855682, 0.12241260836518064]",0.122036291192,"[0.037871995701886992, 0.033881066897856089, 0.04412331753914514, 0.032799301605182143, 0.038714742069742437]",0.0374780847628,"{'num_leaves': 130, 'num_boost_round': 5000, 'lambda_l2': 1.8, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2248, 1: 2123, 2: 1351, 3: 1635, 4: 1424, 5: 849, 6: 631, 7: 694, 8: 2474, 9: 1632, 10: 2856, 11: 0, 12: 2712, 13: 2368, 14: 3186, 15: 3679, 16: 1923, 17: 1762, 18: 2156, 19: 201, 20: 3220, 21: 1474, 22: 2074, 23: 2604, 24: 2848, 25: 2811, 26: 3050, 27: 3027, 28: 4596, 29: 4723, 30: 3961, 31: 3819, 32: 807, 33: 1118, 34: 603, 35: 320, 36: 474, 37: 584, 38: 669, 39: 850, 40: 1066, 41: 925, 42: 814, 43: 3098, 44: 4048, 45: 3984, 46: 2346, 47: 731, 48: 1451, 49: 673, 50: 1434, 51: 497, 52: 1506, 53: 324, 54: 1594, 55: 1825, 56: 781, 57: 1040, 58: 2316, 59: 362, 60: 456, 61: 625, 62: 2006, 63: 947, 64: 421, 65: 1560, 66: 1830, 67: 1086, 68: 2238, 69: 1406, 70: 1898, 71: 1153, 72: 1423, 73: 3729, 74: 2028, 75: 227, 76: 239, 77: 219, 78: 248, 79: 277, 80: 215, 81: 59, 82: 113, 83: 9, 84: 33, 85: 33, 86: 94, 87: 121, 88: 21, 89: 7, 90: 35, 91: 112, 92: 17, 93: 227, 94: 176, 95: 383, 96: 0, 97: 0, 98: 0, 99: 0, 100: 465, 101: 445, 102: 238, 103: 205, 104: 1300, 105: 1285, 106: 825, 107: 637, 108: 1277, 109: 1008, 110: 566, 111: 553, 112: 766, 113: 872, 114: 506, 115: 397, 116: 635, 117: 519, 118: 667, 119: 691, 120: 67, 121: 40, 122: 1206, 123: 898, 124: 1016, 125: 1111, 126: 466, 127: 720, 128: 241, 129: 123, 130: 762, 131: 741, 132: 52, 133: 95, 134: 1263, 135: 1193, 136: 790, 137: 904, 138: 187, 139: 236, 140: 1084, 141: 993, 142: 839, 143: 972, 144: 1094, 145: 1138, 146: 697, 147: 632, 148: 1245, 149: 1125, 150: 530, 151: 581, 152: 497, 153: 512, 154: 365, 155: 336, 156: 831, 157: 753}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_16-16-55,lightgb,"[0.12177503646827677, 0.11881401274635563, 0.12907471117110902, 0.11761381459648422, 0.12251175475704121]",0.121957865948,"[0.039473333913199606, 0.035490475920193679, 0.047141193101107454, 0.032800620186024601, 0.042499851034460148]",0.039481094831,"{'num_leaves': 130, 'num_boost_round': 5000, 'lambda_l2': 1.9, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2019, 1: 1914, 2: 1291, 3: 1473, 4: 1286, 5: 744, 6: 609, 7: 647, 8: 2193, 9: 1491, 10: 2654, 11: 0, 12: 2499, 13: 2201, 14: 2947, 15: 3321, 16: 1706, 17: 1627, 18: 1876, 19: 179, 20: 3006, 21: 1291, 22: 1930, 23: 2376, 24: 2663, 25: 2583, 26: 2750, 27: 2795, 28: 4272, 29: 4383, 30: 3723, 31: 3403, 32: 737, 33: 1058, 34: 592, 35: 317, 36: 432, 37: 522, 38: 627, 39: 810, 40: 972, 41: 859, 42: 783, 43: 2904, 44: 3740, 45: 3634, 46: 2143, 47: 712, 48: 1298, 49: 666, 50: 1320, 51: 498, 52: 1371, 53: 311, 54: 1487, 55: 1555, 56: 707, 57: 972, 58: 2093, 59: 371, 60: 424, 61: 599, 62: 1909, 63: 915, 64: 399, 65: 1508, 66: 1659, 67: 1029, 68: 2077, 69: 1296, 70: 1635, 71: 1011, 72: 1263, 73: 3253, 74: 1933, 75: 216, 76: 185, 77: 174, 78: 230, 79: 298, 80: 188, 81: 64, 82: 106, 83: 4, 84: 34, 85: 28, 86: 93, 87: 88, 88: 19, 89: 7, 90: 42, 91: 96, 92: 23, 93: 213, 94: 141, 95: 353, 96: 0, 97: 0, 98: 0, 99: 0, 100: 412, 101: 391, 102: 212, 103: 182, 104: 1133, 105: 1100, 106: 760, 107: 525, 108: 1147, 109: 932, 110: 520, 111: 452, 112: 697, 113: 742, 114: 447, 115: 341, 116: 557, 117: 462, 118: 632, 119: 659, 120: 67, 121: 35, 122: 1068, 123: 730, 124: 857, 125: 1045, 126: 403, 127: 626, 128: 228, 129: 101, 130: 690, 131: 720, 132: 68, 133: 69, 134: 1094, 135: 1150, 136: 728, 137: 827, 138: 174, 139: 202, 140: 970, 141: 838, 142: 751, 143: 853, 144: 946, 145: 1002, 146: 609, 147: 585, 148: 1092, 149: 994, 150: 482, 151: 502, 152: 437, 153: 460, 154: 329, 155: 323, 156: 785, 157: 736}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_16-26-43,lightgb,"[0.12178374314901823, 0.11889350237766425, 0.12910902525676887, 0.11757536425965619, 0.12280197004164009]",0.122032721017,"[0.038780800830772837, 0.032698196003865296, 0.043153051148733107, 0.032715825436984897, 0.038622314058552168]",0.0371940374958,"{'num_leaves': 130, 'num_boost_round': 5000, 'lambda_l2': 2, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2271, 1: 2079, 2: 1455, 3: 1669, 4: 1455, 5: 879, 6: 673, 7: 724, 8: 2472, 9: 1564, 10: 2929, 11: 0, 12: 2804, 13: 2391, 14: 3276, 15: 3680, 16: 1916, 17: 1866, 18: 2159, 19: 202, 20: 3243, 21: 1453, 22: 2200, 23: 2673, 24: 2974, 25: 2871, 26: 3105, 27: 3086, 28: 4663, 29: 4766, 30: 4060, 31: 3799, 32: 803, 33: 1127, 34: 610, 35: 332, 36: 464, 37: 539, 38: 651, 39: 866, 40: 1076, 41: 931, 42: 843, 43: 3182, 44: 4124, 45: 4056, 46: 2322, 47: 755, 48: 1501, 49: 661, 50: 1518, 51: 521, 52: 1502, 53: 357, 54: 1556, 55: 1723, 56: 801, 57: 1104, 58: 2284, 59: 394, 60: 456, 61: 619, 62: 2042, 63: 969, 64: 431, 65: 1572, 66: 1890, 67: 1093, 68: 2261, 69: 1496, 70: 1868, 71: 1179, 72: 1412, 73: 3771, 74: 2029, 75: 226, 76: 222, 77: 216, 78: 260, 79: 280, 80: 193, 81: 67, 82: 112, 83: 10, 84: 36, 85: 31, 86: 97, 87: 104, 88: 24, 89: 9, 90: 45, 91: 104, 92: 19, 93: 227, 94: 173, 95: 379, 96: 0, 97: 0, 98: 0, 99: 0, 100: 448, 101: 418, 102: 212, 103: 229, 104: 1277, 105: 1298, 106: 791, 107: 642, 108: 1241, 109: 1030, 110: 606, 111: 546, 112: 772, 113: 854, 114: 471, 115: 406, 116: 634, 117: 540, 118: 678, 119: 764, 120: 66, 121: 38, 122: 1184, 123: 831, 124: 1070, 125: 1069, 126: 495, 127: 726, 128: 261, 129: 131, 130: 779, 131: 788, 132: 65, 133: 96, 134: 1239, 135: 1205, 136: 801, 137: 935, 138: 187, 139: 218, 140: 1149, 141: 947, 142: 827, 143: 994, 144: 1136, 145: 1124, 146: 675, 147: 645, 148: 1255, 149: 1125, 150: 474, 151: 571, 152: 486, 153: 524, 154: 361, 155: 362, 156: 878, 157: 822}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_16-36-02,lightgb,"[0.12176269183234374, 0.11918054497916192, 0.12893657747575835, 0.11775757526256272, 0.12275037854073896]",0.122077553618,"[0.039778742309720574, 0.033261068381701939, 0.039885488940587244, 0.033375416422554219, 0.03725437786897523]",0.0367110187847,"{'num_leaves': 140, 'num_boost_round': 5000, 'lambda_l2': 1.8, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2313, 1: 2177, 2: 1469, 3: 1568, 4: 1486, 5: 910, 6: 621, 7: 721, 8: 2551, 9: 1675, 10: 3002, 11: 0, 12: 2859, 13: 2384, 14: 3298, 15: 3801, 16: 2022, 17: 1888, 18: 2191, 19: 190, 20: 3286, 21: 1527, 22: 2203, 23: 2731, 24: 3007, 25: 2945, 26: 3131, 27: 3049, 28: 4696, 29: 4749, 30: 4198, 31: 3929, 32: 849, 33: 1186, 34: 606, 35: 330, 36: 450, 37: 563, 38: 700, 39: 846, 40: 1053, 41: 935, 42: 870, 43: 3190, 44: 4227, 45: 4162, 46: 2460, 47: 751, 48: 1549, 49: 677, 50: 1554, 51: 555, 52: 1512, 53: 339, 54: 1577, 55: 1829, 56: 815, 57: 1069, 58: 2324, 59: 400, 60: 462, 61: 616, 62: 2130, 63: 947, 64: 425, 65: 1676, 66: 1918, 67: 1103, 68: 2346, 69: 1488, 70: 1857, 71: 1202, 72: 1478, 73: 3888, 74: 1999, 75: 212, 76: 227, 77: 183, 78: 265, 79: 293, 80: 208, 81: 61, 82: 136, 83: 4, 84: 35, 85: 26, 86: 103, 87: 116, 88: 18, 89: 12, 90: 39, 91: 107, 92: 30, 93: 186, 94: 184, 95: 379, 96: 0, 97: 0, 98: 0, 99: 0, 100: 455, 101: 459, 102: 224, 103: 200, 104: 1351, 105: 1275, 106: 837, 107: 607, 108: 1251, 109: 1049, 110: 599, 111: 513, 112: 724, 113: 866, 114: 502, 115: 401, 116: 664, 117: 572, 118: 684, 119: 704, 120: 68, 121: 32, 122: 1239, 123: 910, 124: 998, 125: 1163, 126: 461, 127: 698, 128: 230, 129: 140, 130: 780, 131: 788, 132: 63, 133: 103, 134: 1251, 135: 1275, 136: 793, 137: 938, 138: 188, 139: 223, 140: 1117, 141: 947, 142: 878, 143: 1065, 144: 1109, 145: 1153, 146: 681, 147: 662, 148: 1290, 149: 1176, 150: 545, 151: 594, 152: 520, 153: 539, 154: 381, 155: 338, 156: 886, 157: 783}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_16-45-25,lightgb,"[0.12164235995291058, 0.11921949656071364, 0.1292374651973463, 0.11786351290108553, 0.12260240326755624]",0.122113047576,"[0.03590595024245239, 0.03449063751500045, 0.039854156040255363, 0.033522294936391347, 0.039028185728590244]",0.0365602448925,"{'num_leaves': 140, 'num_boost_round': 5000, 'lambda_l2': 1.9, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2238, 1: 2004, 2: 1408, 3: 1604, 4: 1433, 5: 844, 6: 622, 7: 652, 8: 2427, 9: 1606, 10: 2905, 11: 0, 12: 2718, 13: 2326, 14: 3164, 15: 3652, 16: 1940, 17: 1785, 18: 2158, 19: 208, 20: 3178, 21: 1426, 22: 2100, 23: 2652, 24: 2828, 25: 2821, 26: 3023, 27: 2996, 28: 4603, 29: 4629, 30: 4085, 31: 3735, 32: 782, 33: 1135, 34: 608, 35: 335, 36: 474, 37: 534, 38: 682, 39: 826, 40: 1041, 41: 924, 42: 830, 43: 3174, 44: 4092, 45: 4005, 46: 2262, 47: 790, 48: 1470, 49: 652, 50: 1453, 51: 518, 52: 1447, 53: 331, 54: 1494, 55: 1745, 56: 774, 57: 1087, 58: 2314, 59: 371, 60: 458, 61: 604, 62: 2001, 63: 951, 64: 431, 65: 1590, 66: 1879, 67: 1124, 68: 2252, 69: 1397, 70: 1776, 71: 1161, 72: 1365, 73: 3633, 74: 1941, 75: 218, 76: 186, 77: 193, 78: 260, 79: 279, 80: 180, 81: 58, 82: 110, 83: 6, 84: 26, 85: 26, 86: 85, 87: 105, 88: 19, 89: 10, 90: 40, 91: 98, 92: 18, 93: 206, 94: 158, 95: 369, 96: 0, 97: 0, 98: 0, 99: 0, 100: 439, 101: 396, 102: 217, 103: 212, 104: 1219, 105: 1212, 106: 798, 107: 569, 108: 1273, 109: 958, 110: 549, 111: 512, 112: 727, 113: 817, 114: 431, 115: 392, 116: 634, 117: 542, 118: 723, 119: 680, 120: 69, 121: 38, 122: 1191, 123: 844, 124: 946, 125: 1078, 126: 465, 127: 658, 128: 251, 129: 110, 130: 725, 131: 732, 132: 76, 133: 91, 134: 1150, 135: 1186, 136: 768, 137: 896, 138: 175, 139: 222, 140: 1039, 141: 953, 142: 805, 143: 948, 144: 1104, 145: 1107, 146: 635, 147: 588, 148: 1222, 149: 1091, 150: 513, 151: 560, 152: 478, 153: 487, 154: 364, 155: 339, 156: 798, 157: 754}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
30-01-2018_16-54-58,lightgb,"[0.12164438210911989, 0.1188910087297403, 0.12873995765065219, 0.11769234554897678, 0.12274352610871016]",0.121942244029,"[0.037572243939327704, 0.035150636326534458, 0.039126156272669174, 0.031040135984063841, 0.037547099360605171]",0.0360872543766,"{'num_leaves': 140, 'num_boost_round': 5000, 'lambda_l2': 2, 'learning_rate': 0.01, 'early_stopping_rounds': 50, 'boosting': 'gbdt', 'objective': 'binary', 'bagging_fraction_seed': 2017, 'colsample_bytree': 1, 'subsample': 1, 'verbose': 1, 'device': 'cpu', 'metric': 'binary_logloss', 'max_depth': 25, 'feature_fraction': 0.486, 'min_data_in_leaf': 15, 'feature_fraction_seed': 2017}","{'importance': {0: 2293, 1: 2166, 2: 1487, 3: 1704, 4: 1533, 5: 893, 6: 661, 7: 703, 8: 2456, 9: 1710, 10: 3055, 11: 0, 12: 2777, 13: 2399, 14: 3305, 15: 3797, 16: 1969, 17: 1875, 18: 2236, 19: 201, 20: 3286, 21: 1476, 22: 2168, 23: 2670, 24: 2973, 25: 2897, 26: 3161, 27: 3159, 28: 4698, 29: 4757, 30: 4221, 31: 3945, 32: 844, 33: 1134, 34: 616, 35: 340, 36: 465, 37: 542, 38: 680, 39: 876, 40: 1072, 41: 969, 42: 839, 43: 3179, 44: 4255, 45: 4149, 46: 2387, 47: 811, 48: 1457, 49: 721, 50: 1499, 51: 512, 52: 1540, 53: 347, 54: 1646, 55: 1759, 56: 781, 57: 1076, 58: 2412, 59: 375, 60: 485, 61: 614, 62: 2035, 63: 967, 64: 416, 65: 1601, 66: 1881, 67: 1094, 68: 2319, 69: 1539, 70: 1845, 71: 1185, 72: 1483, 73: 3787, 74: 1996, 75: 212, 76: 196, 77: 190, 78: 248, 79: 304, 80: 215, 81: 62, 82: 120, 83: 7, 84: 38, 85: 36, 86: 95, 87: 108, 88: 17, 89: 9, 90: 48, 91: 112, 92: 15, 93: 214, 94: 162, 95: 373, 96: 0, 97: 0, 98: 0, 99: 0, 100: 419, 101: 425, 102: 220, 103: 213, 104: 1328, 105: 1297, 106: 842, 107: 646, 108: 1305, 109: 1036, 110: 573, 111: 545, 112: 751, 113: 832, 114: 498, 115: 386, 116: 643, 117: 540, 118: 705, 119: 686, 120: 70, 121: 33, 122: 1208, 123: 921, 124: 1033, 125: 1179, 126: 483, 127: 717, 128: 240, 129: 123, 130: 734, 131: 792, 132: 60, 133: 98, 134: 1270, 135: 1295, 136: 824, 137: 924, 138: 188, 139: 214, 140: 1140, 141: 1030, 142: 881, 143: 1025, 144: 1071, 145: 1156, 146: 649, 147: 702, 148: 1306, 149: 1132, 150: 533, 151: 573, 152: 487, 153: 530, 154: 367, 155: 364, 156: 851, 157: 804}, 'column': {0: 'len_q1', 1: 'len_q2', 2: 'diff_len', 3: 'len_char_q1', 4: 'len_char_q2', 5: 'len_word_q1', 6: 'len_word_q2', 7: 'common_words', 8: 'fuzz_qratio', 9: 'fuzz_WRatio', 10: 'fuzz_partial_ratio', 11: 'fuzz_partial_token_set_ratio', 12: 'fuzz_partial_token_sort_ratio', 13: 'fuzz_token_set_ratio', 14: 'fuzz_token_sort_ratio', 15: 'wmd', 16: 'norm_wmd', 17: 'cosine_distance', 18: 'cityblock_distance', 19: 'jaccard_distance', 20: 'canberra_distance', 21: 'euclidean_distance', 22: 'minkowski_distance', 23: 'braycurtis_distance', 24: 'skew_q1vec', 25: 'skew_q2vec', 26: 'kur_q1vec', 27: 'kur_q2vec', 28: 'q1_pr', 29: 'q2_pr', 30: 'q1_hash', 31: 'q2_hash', 32: 'q1_freq', 33: 'q2_freq', 34: 'q1_q2_intersect', 35: 'core1', 36: 'core2', 37: 'core3', 38: 'q1_kcores', 39: 'q2_kcores', 40: 'q1_q2_kcores_ratio', 41: 'q1_q2_kcores_diff', 42: 'q1_q2_kcores_diff_normed', 43: 'word_match', 44: 'tfidf_wm', 45: 'tfidf_wm_stops', 46: 'jaccard', 47: 'wc_diff', 48: 'wc_ratio', 49: 'wc_diff_unique', 50: 'wc_ratio_unique', 51: 'wc_diff_unq_stop', 52: 'wc_ratio_unique_stop', 53: 'same_start', 54: 'char_diff', 55: 'char_diff_unq_stop', 56: 'total_unique_words', 57: 'total_unq_words_stop', 58: 'char_ratio', 59: 'q1_neigh', 60: 'q2_neigh', 61: 'common_neigh', 62: 'distinct_neigh', 63: 'clique_size', 64: 'shortest_path', 65: 'bigram_coocurence', 66: 'bigram_distinct', 67: 'bigram_nostpwrd_coocurence', 68: 'bigram_nostpwrd_distinct', 69: '3gram_cooccurence', 70: '3gram_distinct', 71: '3gram_nostpwrd_cooccurence', 72: '3gram_nostpwrd_distinct', 73: 'spacy_similarity', 74: 'shortest_path_weighted', 75: 'q1_how', 76: 'q2_how', 77: 'how_both', 78: 'q1_what', 79: 'q2_what', 80: 'what_both', 81: 'q1_which', 82: 'q2_which', 83: 'which_both', 84: 'q1_who', 85: 'q2_who', 86: 'who_both', 87: 'q1_where', 88: 'q2_where', 89: 'where_both', 90: 'q1_when', 91: 'q2_when', 92: 'when_both', 93: 'q1_why', 94: 'q2_why', 95: 'why_both', 96: 'caps_count_q1', 97: 'caps_count_q2', 98: 'diff_caps', 99: 'exactly_same', 100: 'num_space_q1', 101: 'num_space_q2', 102: 'num_word_q1', 103: 'num_word_q2', 104: 'num_vowels_q1', 105: 'num_vowels_q2', 106: 'num_c_q1', 107: 'num_c_q2', 108: 'num_s_q1', 109: 'num_s_q2', 110: 'num_w_q1', 111: 'num_w_q2', 112: 'num_u_q1', 113: 'num_u_q2', 114: 'num_v_q1', 115: 'num_v_q2', 116: 'num_g_q1', 117: 'num_g_q2', 118: 'num_m_q1', 119: 'num_m_q2', 120: 'num_z_q1', 121: 'num_z_q2', 122: 'num_n_q1', 123: 'num_n_q2', 124: 'num_o_q1', 125: 'num_o_q2', 126: 'num_b_q1', 127: 'num_b_q2', 128: 'num_x_q1', 129: 'num_x_q2', 130: 'num_h_q1', 131: 'num_h_q2', 132: 'num_j_q1', 133: 'num_j_q2', 134: 'num_e_q1', 135: 'num_e_q2', 136: 'num_d_q1', 137: 'num_d_q2', 138: 'num_q_q1', 139: 'num_q_q2', 140: 'num_i_q1', 141: 'num_i_q2', 142: 'num_r_q1', 143: 'num_r_q2', 144: 'num_t_q1', 145: 'num_t_q2', 146: 'num_y_q1', 147: 'num_y_q2', 148: 'num_a_q1', 149: 'num_a_q2', 150: 'num_p_q1', 151: 'num_p_q2', 152: 'num_f_q1', 153: 'num_f_q2', 154: 'num_k_q1', 155: 'num_k_q2', 156: 'num_l_q1', 157: 'num_l_q2'}}"
