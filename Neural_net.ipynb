{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, concatenate, Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from log import _check_log_directory,_initialise_model_log\n",
    "from operator import itemgetter\n",
    "from keras.utils import plot_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "log_dir = './log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove features\n",
    "features_train = pd.read_csv(data_dir+'train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv(data_dir+'test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_train= features_train.drop(['question1', 'question2'], axis=1)\n",
    "features_test = features_test.drop(['id','qid1','qid2','question1', 'question2'], axis=1)\n",
    "data_train = pd.read_csv(data_dir+'train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "\n",
    "# Pagerank features\n",
    "pagerank_feats_train = pd.read_csv(data_dir+\"train_pagerank.csv\", sep=',')\n",
    "pagerank_feats_test = pd.read_csv(data_dir+\"test_pagerank.csv\", sep=',')\n",
    "\n",
    "# Question frequency\n",
    "train_question_freq = pd.read_csv(data_dir+'train_question_freq.csv', sep=',', index_col=0)\n",
    "test_question_freq = pd.read_csv(data_dir+'test_question_freq.csv', sep=',', index_col=0)\n",
    "\n",
    "# Intersection of questions\n",
    "train_question_inter= pd.read_csv(data_dir+'train_question_inter.csv', sep=',', index_col=0)\n",
    "test_question_inter = pd.read_csv(data_dir+'test_question_inter.csv', sep=',', index_col=0)\n",
    "\n",
    "# K-cores\n",
    "train_kcores = pd.read_csv(data_dir+'train_kcores.csv', sep=',', index_col=0)\n",
    "test_kcores = pd.read_csv(data_dir+'test_kcores.csv', sep=',', index_col=0)\n",
    "\n",
    "# question K-cores\n",
    "train_question_kcores = pd.read_csv(data_dir+'train_question_kcores.csv', sep=',', index_col=0)\n",
    "test_question_kcores = pd.read_csv(data_dir+'test_question_kcores.csv', sep=',', index_col=0)\n",
    "\n",
    "# TF-IDF\n",
    "train_tfidf = pd.read_csv(data_dir+'train_tfidf.csv', sep=',', index_col=0)\n",
    "test_tfidf = pd.read_csv(data_dir+'test_tfidf.csv', sep=',', index_col=0)\n",
    "\n",
    "# Graph features\n",
    "train_graph_feat = pd.read_csv(data_dir+'train_graph_feat.csv', sep=',', index_col=0)\n",
    "test_graph_feat = pd.read_csv(data_dir+'test_graph_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# Bigram feature\n",
    "train_bigram_feat = pd.read_csv(data_dir+'train_2gram_feat.csv', sep=',', index_col=0)\n",
    "test_bigram_feat = pd.read_csv(data_dir+'test_2gram_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# 3gram feature\n",
    "train_3gram_feat = pd.read_csv(data_dir+'train_3gram_feat.csv', sep=',', index_col=0)\n",
    "test_3gram_feat = pd.read_csv(data_dir+'test_3gram_feat.csv', sep=',', index_col=0)\n",
    "\n",
    "# spaCy feature\n",
    "train_spacy_feat = pd.read_csv(data_dir+'train_spacy_features.csv', sep=',', index_col=0)\n",
    "test_spacy_feat = pd.read_csv(data_dir+'test_spacy_features.csv', sep=',', index_col=0)\n",
    "\n",
    "# Graph features2 NE PAS PRENDRE !!!\n",
    "#train_graph_feat2 = pd.read_csv(data_dir+'train_graph_feat2.csv', sep=',', index_col=0)\n",
    "#test_graph_feat2 = pd.read_csv(data_dir+'test_graph_feat2.csv', sep=',', index_col=0)\n",
    "\n",
    "# Word features\n",
    "train_word_feat = pd.read_csv(data_dir+'train_word_feat.csv', sep=',', index_col=0)\n",
    "test_word_feat = pd.read_csv(data_dir+'test_word_feat.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pagerank features\n",
    "features_train[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_train[[\"q1_pr\",\"q2_pr\"]]\n",
    "features_test[[\"q1_pr\",\"q2_pr\"]]=pagerank_feats_test[[\"q1_pr\",\"q2_pr\"]]\n",
    "\n",
    "# Add question frequency features\n",
    "features_train[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=train_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "features_test[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]=test_question_freq[[\"q1_hash\",\"q2_hash\",\"q1_freq\",\"q2_freq\"]]\n",
    "\n",
    "# Add intersection of questions features\n",
    "features_train['q1_q2_intersect']=train_question_inter['q1_q2_intersect']\n",
    "features_test['q1_q2_intersect']=test_question_inter['q1_q2_intersect']\n",
    "\n",
    "# Add K-cores\n",
    "features_train[[\"core1\",\"core2\",\"core3\"]] = train_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "features_test[[\"core1\",\"core2\",\"core3\"]] = test_kcores[[\"core1\",\"core2\",\"core3\"]]\n",
    "\n",
    "# Add question K-cores features\n",
    "features_train[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "                'q1_q2_kcores_diff_normed']]=train_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "features_test[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', \n",
    "               'q1_q2_kcores_diff_normed']]=test_question_kcores[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "\n",
    "# Add TF-IDF features\n",
    "features_train[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=train_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "features_test[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=test_tfidf[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "\n",
    "# Add graph features\n",
    "features_train[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']] = train_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']]\n",
    "features_test[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']] = test_graph_feat[['q1_neigh','q2_neigh','common_neigh', 'distinct_neigh', 'clique_size', 'shortest_path']]\n",
    "\n",
    "# Add bigram features\n",
    "features_train[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']] = train_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "features_test[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']] = test_bigram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "\n",
    "# Add 3gram features\n",
    "features_train[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']] = train_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "features_test[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']] = test_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "\n",
    "# Add spaCy features\n",
    "features_train[['spacy_similarity']] = train_spacy_feat[['spacy_similarity']]\n",
    "features_test[['spacy_similarity']] = test_spacy_feat[['spacy_similarity']]\n",
    "\n",
    "# Add graph features2\n",
    "#features_train[['shortest_path']] = train_graph_feat2[['shortest_path']]\n",
    "#features_test[['shortest_path']] = test_graph_feat2[['shortest_path']]\n",
    "\n",
    "# Add graph features\n",
    "features_train[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]=train_word_feat[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]\n",
    "features_test[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]=test_word_feat[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_train\n",
    "X_test = features_test\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(value=0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.fillna(value=0)\n",
    "\n",
    "\n",
    "X = features_train.drop(['is_duplicate'],axis=1)\n",
    "X_test = features_test\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X .fillna(value=0)\n",
    "X_test=X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test=X_test.fillna(value=0)\n",
    "Y = data_train[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                       u'len_q1',                        u'len_q2',\n",
       "                            u'diff_len',                   u'len_char_q1',\n",
       "                         u'len_char_q2',                   u'len_word_q1',\n",
       "                         u'len_word_q2',                  u'common_words',\n",
       "                         u'fuzz_qratio',                   u'fuzz_WRatio',\n",
       "                  u'fuzz_partial_ratio',  u'fuzz_partial_token_set_ratio',\n",
       "       u'fuzz_partial_token_sort_ratio',          u'fuzz_token_set_ratio',\n",
       "               u'fuzz_token_sort_ratio',                           u'wmd',\n",
       "                            u'norm_wmd',               u'cosine_distance',\n",
       "                  u'cityblock_distance',              u'jaccard_distance',\n",
       "                   u'canberra_distance',            u'euclidean_distance',\n",
       "                  u'minkowski_distance',           u'braycurtis_distance',\n",
       "                          u'skew_q1vec',                    u'skew_q2vec',\n",
       "                           u'kur_q1vec',                     u'kur_q2vec',\n",
       "                               u'q1_pr',                         u'q2_pr',\n",
       "                             u'q1_hash',                       u'q2_hash',\n",
       "                             u'q1_freq',                       u'q2_freq',\n",
       "                     u'q1_q2_intersect',                         u'core1',\n",
       "                               u'core2',                         u'core3',\n",
       "                           u'q1_kcores',                     u'q2_kcores',\n",
       "                  u'q1_q2_kcores_ratio',             u'q1_q2_kcores_diff',\n",
       "            u'q1_q2_kcores_diff_normed',                    u'word_match',\n",
       "                            u'tfidf_wm',                u'tfidf_wm_stops',\n",
       "                             u'jaccard',                       u'wc_diff',\n",
       "                            u'wc_ratio',                u'wc_diff_unique',\n",
       "                     u'wc_ratio_unique',              u'wc_diff_unq_stop',\n",
       "                u'wc_ratio_unique_stop',                    u'same_start',\n",
       "                           u'char_diff',            u'char_diff_unq_stop',\n",
       "                  u'total_unique_words',          u'total_unq_words_stop',\n",
       "                          u'char_ratio',                      u'q1_neigh',\n",
       "                            u'q2_neigh',                  u'common_neigh',\n",
       "                      u'distinct_neigh',                   u'clique_size',\n",
       "                       u'shortest_path',             u'bigram_coocurence',\n",
       "                     u'bigram_distinct',    u'bigram_nostpwrd_coocurence',\n",
       "            u'bigram_nostpwrd_distinct',             u'3gram_cooccurence',\n",
       "                      u'3gram_distinct',    u'3gram_nostpwrd_cooccurence',\n",
       "             u'3gram_nostpwrd_distinct',              u'spacy_similarity',\n",
       "                              u'q1_how',                        u'q2_how',\n",
       "                            u'how_both',                       u'q1_what',\n",
       "                             u'q2_what',                     u'what_both',\n",
       "                            u'q1_which',                      u'q2_which',\n",
       "                          u'which_both',                        u'q1_who',\n",
       "                              u'q2_who',                      u'who_both',\n",
       "                            u'q1_where',                      u'q2_where',\n",
       "                          u'where_both',                       u'q1_when',\n",
       "                             u'q2_when',                     u'when_both',\n",
       "                              u'q1_why',                        u'q2_why',\n",
       "                            u'why_both',                 u'caps_count_q1',\n",
       "                       u'caps_count_q2',                     u'diff_caps',\n",
       "                        u'exactly_same'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3811"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X\n",
    "test_size = 0.1\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, Y, test_size=test_size)\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features = 99\n"
     ]
    }
   ],
   "source": [
    "print \"number of features =\", X_train.values.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First training phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/20\n",
      "71936/72090 [============================>.] - ETA: 0s - loss: 0.5716Epoch 00001: val_loss improved from inf to 0.51703, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 13s 187us/step - loss: 0.5714 - val_loss: 0.5170\n",
      "Epoch 2/20\n",
      "72000/72090 [============================>.] - ETA: 0s - loss: 0.4666Epoch 00002: val_loss improved from 0.51703 to 0.39701, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00002: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 12s 172us/step - loss: 0.4665 - val_loss: 0.3970\n",
      "Epoch 3/20\n",
      "72064/72090 [============================>.] - ETA: 0s - loss: 0.3696Epoch 00003: val_loss improved from 0.39701 to 0.32008, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 16s 216us/step - loss: 0.3696 - val_loss: 0.3201\n",
      "Epoch 4/20\n",
      "71776/72090 [============================>.] - ETA: 0s - loss: 0.3138Epoch 00004: val_loss improved from 0.32008 to 0.30622, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00004: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 15s 209us/step - loss: 0.3138 - val_loss: 0.3062\n",
      "Epoch 5/20\n",
      "71872/72090 [============================>.] - ETA: 0s - loss: 0.2934Epoch 00005: val_loss improved from 0.30622 to 0.29813, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 18s 244us/step - loss: 0.2933 - val_loss: 0.2981\n",
      "Epoch 6/20\n",
      "71840/72090 [============================>.] - ETA: 0s - loss: 0.2897Epoch 00006: val_loss improved from 0.29813 to 0.27878, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00006: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 21s 295us/step - loss: 0.2897 - val_loss: 0.2788\n",
      "Epoch 7/20\n",
      "71904/72090 [============================>.] - ETA: 0s - loss: 0.2767Epoch 00007: val_loss did not improve\n",
      "72090/72090 [==============================] - 21s 289us/step - loss: 0.2767 - val_loss: 0.3168\n",
      "Epoch 8/20\n",
      "72000/72090 [============================>.] - ETA: 0s - loss: 0.2779Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 17s 232us/step - loss: 0.2780 - val_loss: 0.3368\n",
      "Epoch 9/20\n",
      "72000/72090 [============================>.] - ETA: 0s - loss: 0.2775Epoch 00009: val_loss improved from 0.27878 to 0.26040, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 17s 230us/step - loss: 0.2774 - val_loss: 0.2604\n",
      "Epoch 10/20\n",
      "71936/72090 [============================>.] - ETA: 0s - loss: 0.2731Epoch 00010: val_loss did not improve\n",
      "Epoch 00010: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 18s 250us/step - loss: 0.2731 - val_loss: 0.2621\n",
      "Epoch 11/20\n",
      "72000/72090 [============================>.] - ETA: 0s - loss: 0.2709Epoch 00011: val_loss did not improve\n",
      "72090/72090 [==============================] - 20s 275us/step - loss: 0.2709 - val_loss: 0.3061\n",
      "Epoch 12/20\n",
      "72064/72090 [============================>.] - ETA: 0s - loss: 0.2657Epoch 00012: val_loss did not improve\n",
      "Epoch 00012: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 20s 273us/step - loss: 0.2658 - val_loss: 0.2895\n",
      "Epoch 13/20\n",
      "71840/72090 [============================>.] - ETA: 0s - loss: 0.2684Epoch 00013: val_loss did not improve\n",
      "72090/72090 [==============================] - 21s 292us/step - loss: 0.2682 - val_loss: 0.2608\n",
      "Epoch 14/20\n",
      "71904/72090 [============================>.] - ETA: 0s - loss: 0.2715Epoch 00014: val_loss improved from 0.26040 to 0.25913, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00014: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 22s 299us/step - loss: 0.2715 - val_loss: 0.2591\n",
      "Epoch 15/20\n",
      "72000/72090 [============================>.] - ETA: 0s - loss: 0.2665Epoch 00015: val_loss improved from 0.25913 to 0.25632, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 18s 243us/step - loss: 0.2665 - val_loss: 0.2563\n",
      "Epoch 16/20\n",
      "71808/72090 [============================>.] - ETA: 0s - loss: 0.2686Epoch 00016: val_loss did not improve\n",
      "Epoch 00016: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 14s 188us/step - loss: 0.2685 - val_loss: 0.2616\n",
      "Epoch 17/20\n",
      "72032/72090 [============================>.] - ETA: 0s - loss: 0.2632Epoch 00017: val_loss did not improve\n",
      "72090/72090 [==============================] - 12s 168us/step - loss: 0.2632 - val_loss: 0.2601\n",
      "Epoch 18/20\n",
      "71776/72090 [============================>.] - ETA: 0s - loss: 0.2615Epoch 00018: val_loss did not improve\n",
      "Epoch 00018: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 14s 193us/step - loss: 0.2616 - val_loss: 0.2870\n",
      "Epoch 19/20\n",
      "71776/72090 [============================>.] - ETA: 0s - loss: 0.2614Epoch 00019: val_loss did not improve\n",
      "72090/72090 [==============================] - 13s 174us/step - loss: 0.2615 - val_loss: 0.2711\n",
      "Epoch 20/20\n",
      "72032/72090 [============================>.] - ETA: 0s - loss: 0.2650Epoch 00020: val_loss did not improve\n",
      "Epoch 00020: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 14s 191us/step - loss: 0.2650 - val_loss: 0.2659\n"
     ]
    }
   ],
   "source": [
    "X_train_np = X_train.values\n",
    "X_val_np = X_val.values\n",
    "Y_train_np = Y_train\n",
    "Y_val_np  = Y_val\n",
    "\n",
    "shape = (X_train_np.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='sigmoid',input_shape=shape))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "\n",
    "weights_path = os.path.join(data_dir,'weights_1hidden_layer')\n",
    "weights_path_best = weights_path + \".best.h5\"\n",
    "weights_path_last = weights_path + \".last.h5\"\n",
    "checkpoint_best = ModelCheckpoint(weights_path_best, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "checkpoint_last = ModelCheckpoint(weights_path_last, verbose=1, save_weights_only=True, period=2)\n",
    "callbacks = [checkpoint_best,checkpoint_last]\n",
    "history = model.fit(X_train_np,Y_train_np,epochs=20,validation_data =(X_val_np,Y_val_np),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second training phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/15\n",
      "71872/72090 [============================>.] - ETA: 0s - loss: 0.2374Epoch 00001: val_loss improved from 0.25632 to 0.23391, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 12s 163us/step - loss: 0.2373 - val_loss: 0.2339\n",
      "Epoch 2/15\n",
      "72064/72090 [============================>.] - ETA: 0s - loss: 0.2296Epoch 00002: val_loss improved from 0.23391 to 0.22972, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00002: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 12s 166us/step - loss: 0.2296 - val_loss: 0.2297\n",
      "Epoch 3/15\n",
      "71808/72090 [============================>.] - ETA: 0s - loss: 0.2267Epoch 00003: val_loss improved from 0.22972 to 0.22767, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 12s 162us/step - loss: 0.2268 - val_loss: 0.2277\n",
      "Epoch 4/15\n",
      "71968/72090 [============================>.] - ETA: 0s - loss: 0.2253Epoch 00004: val_loss improved from 0.22767 to 0.22737, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00004: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 11s 151us/step - loss: 0.2253 - val_loss: 0.2274\n",
      "Epoch 5/15\n",
      "71968/72090 [============================>.] - ETA: 0s - loss: 0.2240Epoch 00005: val_loss improved from 0.22737 to 0.22458, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 11s 152us/step - loss: 0.2239 - val_loss: 0.2246\n",
      "Epoch 6/15\n",
      "71776/72090 [============================>.] - ETA: 0s - loss: 0.2241Epoch 00006: val_loss improved from 0.22458 to 0.22389, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00006: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 11s 154us/step - loss: 0.2240 - val_loss: 0.2239\n",
      "Epoch 7/15\n",
      "71936/72090 [============================>.] - ETA: 0s - loss: 0.2228Epoch 00007: val_loss did not improve\n",
      "72090/72090 [==============================] - 11s 151us/step - loss: 0.2227 - val_loss: 0.2252\n",
      "Epoch 8/15\n",
      "72064/72090 [============================>.] - ETA: 0s - loss: 0.2225Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 11s 154us/step - loss: 0.2225 - val_loss: 0.2250\n",
      "Epoch 9/15\n",
      "71840/72090 [============================>.] - ETA: 0s - loss: 0.2218Epoch 00009: val_loss improved from 0.22389 to 0.22306, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 11s 149us/step - loss: 0.2219 - val_loss: 0.2231\n",
      "Epoch 10/15\n",
      "71808/72090 [============================>.] - ETA: 0s - loss: 0.2213Epoch 00010: val_loss did not improve\n",
      "Epoch 00010: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 11s 155us/step - loss: 0.2213 - val_loss: 0.2231\n",
      "Epoch 11/15\n",
      "71776/72090 [============================>.] - ETA: 0s - loss: 0.2214Epoch 00011: val_loss improved from 0.22306 to 0.22128, saving model to data/weights_1hidden_layer.best.h5\n",
      "72090/72090 [==============================] - 11s 152us/step - loss: 0.2216 - val_loss: 0.2213\n",
      "Epoch 12/15\n",
      "71936/72090 [============================>.] - ETA: 0s - loss: 0.2215Epoch 00012: val_loss improved from 0.22128 to 0.22059, saving model to data/weights_1hidden_layer.best.h5\n",
      "Epoch 00012: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 11s 148us/step - loss: 0.2214 - val_loss: 0.2206\n",
      "Epoch 13/15\n",
      "71840/72090 [============================>.] - ETA: 0s - loss: 0.2205Epoch 00013: val_loss did not improve\n",
      "72090/72090 [==============================] - 11s 156us/step - loss: 0.2206 - val_loss: 0.2257\n",
      "Epoch 14/15\n",
      "71904/72090 [============================>.] - ETA: 0s - loss: 0.2205Epoch 00014: val_loss did not improve\n",
      "Epoch 00014: saving model to data/weights_1hidden_layer.last.h5\n",
      "72090/72090 [==============================] - 11s 154us/step - loss: 0.2205 - val_loss: 0.2213\n",
      "Epoch 15/15\n",
      "72032/72090 [============================>.] - ETA: 0s - loss: 0.2198Epoch 00015: val_loss did not improve\n",
      "72090/72090 [==============================] - 11s 154us/step - loss: 0.2198 - val_loss: 0.2228\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "history2 = model.fit(X_train_np,Y_train_np,epochs=15,validation_data =(X_val_np,Y_val_np),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAHjCAYAAAAdVu/iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUFASERERERHJoaAkIiIiIiKS4/8DWM1+qaqnIJ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117839f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (14,8)\n",
    "plt.plot(history.history[\"loss\"]+history2.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"]+history2.history[\"val_loss\"])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_path_best)\n",
    "pred = model.predict([X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save result in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.648567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.394559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.005402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.221550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.994885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Score\n",
       "count  20179.000000\n",
       "mean       0.648567\n",
       "std        0.394559\n",
       "min        0.005402\n",
       "25%        0.221550\n",
       "50%        0.977707\n",
       "75%        0.994885\n",
       "max        1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(pred)\n",
    "df_pred.index = X_test.index\n",
    "df_pred.columns = [\"Score\"]\n",
    "df_pred.to_csv(\"data/submission_file_FCNN_1hidden1024_99features_0,21.csv\",index_label=\"Id\")\n",
    "df_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([28161, 51939]))\n"
     ]
    }
   ],
   "source": [
    "print np.unique(Y,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(data_dir+'test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de la taille maximale des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train = data_train[\"question1\"]\n",
    "q2_train = data_train[\"question2\"]\n",
    "q1_test = data_test[\"question1\"]\n",
    "q2_test = data_test[\"question2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_sp = [q.lower().replace('?','').split() for q in q1_train]\n",
    "q2_train_sp = [q.lower().replace('?','').split() for q in q2_train]\n",
    "q1_test_sp = [q.lower().replace('?','').split() for q in q1_test]\n",
    "q2_test_sp = [q.lower().replace('?','').split() for q in q2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "58\n",
      "73\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print max([len(q) for q in q1_train_sp])\n",
    "print max([len(q) for q in q2_train_sp])\n",
    "print max([len(q) for q in q1_test_sp])\n",
    "print max([len(q) for q in q2_test_sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 73 # maximum word per question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mot dans le vocabulaire de train : 23059\n"
     ]
    }
   ],
   "source": [
    "words = {}\n",
    "for i in range(len(q1_train_sp)):\n",
    "    for q in q1_train_sp[i]:\n",
    "        if q in words:\n",
    "            words[q] += 1\n",
    "        else:\n",
    "            words[q] = 1\n",
    "for i in range(len(q2_train_sp)):\n",
    "    for q in q2_train_sp[i]:\n",
    "        if q in words:\n",
    "            words[q] += 1\n",
    "        else:\n",
    "            words[q] = 1\n",
    "            \n",
    "sorted_words = sorted(words.items(),key=itemgetter(1), reverse=True)\n",
    "words_index ={}\n",
    "for num,couple in enumerate(sorted_words):\n",
    "    words_index[couple[0]] = num + 1\n",
    "print \"Nombre de mot dans le vocabulaire de train :\",len(sorted_words)\n",
    "\n",
    "def get_index(word,vocab):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    return 0\n",
    "q1_train_sp_ind = [ [get_index(word,words_index) for word in sentence] for sentence in q1_train_sp]\n",
    "q2_train_sp_ind = [ [get_index(word,words_index) for word in sentence] for sentence in q2_train_sp]\n",
    "q1_test_sp_ind = [ [get_index(word,words_index) for word in sentence] for sentence in q1_test_sp]\n",
    "q2_test_sp_ind = [ [get_index(word,words_index) for word in sentence] for sentence in q2_test_sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 23059+1 # vocab size + 0 pour ceux qu'on connait pas \n",
    "embedding_dim = 300 # embeding size\n",
    "dropout_rate = 0.5\n",
    "\n",
    "lstm_input1 = Input(shape=(seq_length,), name='lstm_input1')\n",
    "lstm_input2 = Input(shape=(seq_length,), name='lstm_input2')\n",
    "\n",
    "E_layer = Embedding(num_words, embedding_dim, weights=None, input_length=seq_length, trainable=True,mask_zero=True)\n",
    "emb1 = E_layer(lstm_input1)\n",
    "emb2 = E_layer(lstm_input2)\n",
    "\n",
    "lstm_layer = LSTM(units=512,dropout=dropout_rate)\n",
    "lstm1 = lstm_layer(emb1)\n",
    "lstm2 = lstm_layer(emb2)\n",
    "x = concatenate([lstm1,lstm2])\n",
    "x = Dense(1024, activation='sigmoid')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[lstm_input1,lstm_input2],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input1 (InputLayer)        (None, 73)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_input2 (InputLayer)        (None, 73)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 73, 300)      6918000     lstm_input1[0][0]                \n",
      "                                                                 lstm_input2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 512)          1665024     embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            1025        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,633,649\n",
      "Trainable params: 9,633,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,to_file='model_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_sp_ind_pad = sequence.pad_sequences(q1_train_sp_ind, maxlen=seq_length,padding='post')\n",
    "q2_train_sp_ind_pad = sequence.pad_sequences(q2_train_sp_ind, maxlen=seq_length,padding='post')\n",
    "q1_test_sp_ind_pad = sequence.pad_sequences(q1_test_sp_ind, maxlen=seq_length,padding='post')\n",
    "q2_test_sp_ind_pad = sequence.pad_sequences(q2_test_sp_ind, maxlen=seq_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2829"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.hstack([q1_train_sp_ind_pad,q2_train_sp_ind_pad])\n",
    "Y = data_train['is_duplicate']\n",
    "test_size = 0.1\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, Y, test_size=test_size)\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "weights_path = os.path.join(data_dir,'weights_LSTM')\n",
    "weights_path_best = weights_path + \".best.h5\"\n",
    "weights_path_last = weights_path + \".last.h5\"\n",
    "checkpoint_best = ModelCheckpoint(weights_path_best, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "checkpoint_last = ModelCheckpoint(weights_path_last, verbose=1, save_weights_only=True, period=2)\n",
    "callbacks = [checkpoint_best,checkpoint_last]\n",
    "\n",
    "q1_train = X_train[:,:73]\n",
    "q2_train = X_train[:,73:]\n",
    "q1_val = X_val[:,:73]\n",
    "q2_val = X_val[:,73:]\n",
    "\n",
    "history = model.fit([q1_train,q2_train],Y_train,epochs=3,validation_data =([q1_val,q2_val],Y_val),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apres ici ce sont des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 1, 14, 1060, 2846, 488],\n",
       " [3, 11, 5, 108, 295, 527],\n",
       " [2, 4, 1, 14, 37, 6, 222, 24, 491],\n",
       " [18, 4, 17, 118, 3217, 409, 597, 1196, 12, 1358],\n",
       " [2, 4, 1, 14, 37, 6, 269, 41, 342, 599]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_test_sp_ind[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80100, 146)\n",
      "[[   2    9    1   20   12    1   14 1538    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    2    9   20   12    1  851 1538   12  101   60   18\n",
      "     9  103  476    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [   2    9    1  662   29  297   16  384 2366    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    2    9   20 1151  662   63   93    6  147 2366    6\n",
      "   996    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "truc = np.hstack([q1_train_sp_ind_pad,q2_train_sp_ind_pad])\n",
    "print truc.shape\n",
    "print truc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245776</td>\n",
       "      <td>2705</td>\n",
       "      <td>What are the best sites to book a hotel online?</td>\n",
       "      <td>What is the best hotel booking service?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104796</td>\n",
       "      <td>48346</td>\n",
       "      <td>How can I stop masturbation?</td>\n",
       "      <td>How can I stop doing masturbation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41770</td>\n",
       "      <td>383018</td>\n",
       "      <td>Which is the best way to control anger?</td>\n",
       "      <td>What is the best way to control your anger?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81132</td>\n",
       "      <td>401393</td>\n",
       "      <td>Why is my Miniature Pinscher/Chihuahua mix afr...</td>\n",
       "      <td>Why is my Black Lab/Pitbull mix puppy afraid o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>244572</td>\n",
       "      <td>7520</td>\n",
       "      <td>How do I get rid off from porn addiction?</td>\n",
       "      <td>What is the best way to overcome an porn addic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>436197</td>\n",
       "      <td>287812</td>\n",
       "      <td>What is the easiest way to learn how to draw?</td>\n",
       "      <td>How do I learn how to draw?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61578</td>\n",
       "      <td>451982</td>\n",
       "      <td>What secret can you not share with anyone?</td>\n",
       "      <td>What is the one secret you can never share wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>134731</td>\n",
       "      <td>120381</td>\n",
       "      <td>How can I improve my vocabulary?</td>\n",
       "      <td>What are some good ways to improve English voc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>383155</td>\n",
       "      <td>74916</td>\n",
       "      <td>How do I calculate square roots and cubed root...</td>\n",
       "      <td>What is the method to calculate a square root ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>274533</td>\n",
       "      <td>376894</td>\n",
       "      <td>What are the best books to read to learn about...</td>\n",
       "      <td>Which Is the best book on psychology?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>269621</td>\n",
       "      <td>440889</td>\n",
       "      <td>How can I stop spending so much time on the In...</td>\n",
       "      <td>How do I stop wasting my time on the internet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>448829</td>\n",
       "      <td>46013</td>\n",
       "      <td>What is a good free editing software for PC ga...</td>\n",
       "      <td>Which is the best video editing software?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>450096</td>\n",
       "      <td>318304</td>\n",
       "      <td>Why is India performing bad in Olympics?</td>\n",
       "      <td>What is the reason of the poor performance of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>392910</td>\n",
       "      <td>124619</td>\n",
       "      <td>What was the mystery behind Padmanabha Swamy T...</td>\n",
       "      <td>What is the real mystery behind Padmanabhaswam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>372844</td>\n",
       "      <td>159742</td>\n",
       "      <td>Why is Saltwater Taffy candy imported in Portu...</td>\n",
       "      <td>Why is Saltwater taffy candy imported in South...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44306</td>\n",
       "      <td>266477</td>\n",
       "      <td>What programming language I should learn first?</td>\n",
       "      <td>What is the best programming language I should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>214806</td>\n",
       "      <td>256610</td>\n",
       "      <td>How does one ask a question anonymously?</td>\n",
       "      <td>How do I answer questions anonymously on Quora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26659</td>\n",
       "      <td>218948</td>\n",
       "      <td>How imminent is World War three?</td>\n",
       "      <td>Is World War 3 more imminent than expected?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18431</td>\n",
       "      <td>126901</td>\n",
       "      <td>What are your views on ban of 500 and 1000 rup...</td>\n",
       "      <td>What are your views on banning 500 and 1000 ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>97343</td>\n",
       "      <td>387016</td>\n",
       "      <td>Which is the best photo editing app for android?</td>\n",
       "      <td>What is the best photo editing app for Android...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57110</td>\n",
       "      <td>392060</td>\n",
       "      <td>How can one meet British/Irish people in the T...</td>\n",
       "      <td>How can I meet British people in Canada?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>441670</td>\n",
       "      <td>212168</td>\n",
       "      <td>What is the most badass thing anyone has ever ...</td>\n",
       "      <td>What is the most badass thing you have ever do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>288752</td>\n",
       "      <td>244179</td>\n",
       "      <td>Do animals commit suicide?</td>\n",
       "      <td>Can animals think of committing suicide?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26518</td>\n",
       "      <td>62322</td>\n",
       "      <td>In which city in Kerala is famous for honeymoon?</td>\n",
       "      <td>Which is the best resort for honeymoon in kerala?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>300871</td>\n",
       "      <td>188843</td>\n",
       "      <td>How can I watch movies online?</td>\n",
       "      <td>Where can I watch movies online for free?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>378990</td>\n",
       "      <td>263801</td>\n",
       "      <td>Can you get pregnant the day before your perio...</td>\n",
       "      <td>If I have a 6 day period can I get pregnant ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>303408</td>\n",
       "      <td>370315</td>\n",
       "      <td>How will long distance relationship work?</td>\n",
       "      <td>What does it take for a successful long distan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>187628</td>\n",
       "      <td>399080</td>\n",
       "      <td>Would a Donald Trump presidency really be so bad?</td>\n",
       "      <td>What would America look like with Donald Trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>335928</td>\n",
       "      <td>21635</td>\n",
       "      <td>Justin Bieber (musician): Justinbieber what ar...</td>\n",
       "      <td>Can you see who views your Instagram?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>150088</td>\n",
       "      <td>14346</td>\n",
       "      <td>Can a girl get pregnant after her last day of ...</td>\n",
       "      <td>Can I get pregnant 3 days after period if I bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20149</th>\n",
       "      <td>103125</td>\n",
       "      <td>409631</td>\n",
       "      <td>How can I get started using Quora?</td>\n",
       "      <td>How do I get started using Quora? Why all my w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150</th>\n",
       "      <td>314384</td>\n",
       "      <td>65383</td>\n",
       "      <td>How can I convert audible AAX files to MP3 mac?</td>\n",
       "      <td>How can I convert audible AAX files to MP3?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20151</th>\n",
       "      <td>203865</td>\n",
       "      <td>223489</td>\n",
       "      <td>Does sure gell help pass a drug test for meth?</td>\n",
       "      <td>Can hair dye help pass a hair follicle drug te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20152</th>\n",
       "      <td>43855</td>\n",
       "      <td>340063</td>\n",
       "      <td>Does massage really increase breast size?</td>\n",
       "      <td>Do breast massages really help in increasing b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20153</th>\n",
       "      <td>353886</td>\n",
       "      <td>205442</td>\n",
       "      <td>How can one track a mobile number location?</td>\n",
       "      <td>How can I track a mobile number and the locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20154</th>\n",
       "      <td>295838</td>\n",
       "      <td>286664</td>\n",
       "      <td>How did you quit/stop smoking?</td>\n",
       "      <td>How can I stop smoking?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20155</th>\n",
       "      <td>58482</td>\n",
       "      <td>127486</td>\n",
       "      <td>I want to live the rest of my life alone and w...</td>\n",
       "      <td>How do I accept that I will live alone the res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20156</th>\n",
       "      <td>451324</td>\n",
       "      <td>113225</td>\n",
       "      <td>How did Trump win America's vote?</td>\n",
       "      <td>How did Donald Trump win the election?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20157</th>\n",
       "      <td>46094</td>\n",
       "      <td>347874</td>\n",
       "      <td>What is the biggest mistake people do when the...</td>\n",
       "      <td>What are the biggest mistakes people make when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158</th>\n",
       "      <td>367645</td>\n",
       "      <td>287563</td>\n",
       "      <td>What do most Americans (from the US) think of ...</td>\n",
       "      <td>What do American girls think about Indian guys?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>232757</td>\n",
       "      <td>358272</td>\n",
       "      <td>What is the craziest question ever asked on Qu...</td>\n",
       "      <td>What is the strangest question on Quora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160</th>\n",
       "      <td>274892</td>\n",
       "      <td>122148</td>\n",
       "      <td>How can I improve my realistic drawings?</td>\n",
       "      <td>How can I improve my drawing ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>411133</td>\n",
       "      <td>341720</td>\n",
       "      <td>What are some of the best Hollywood movies to ...</td>\n",
       "      <td>Which is the all time best Hollywood movie?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20162</th>\n",
       "      <td>118359</td>\n",
       "      <td>110608</td>\n",
       "      <td>Self-Improvement: How can I motivate myself to...</td>\n",
       "      <td>Why do I lack motivation to work hard?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20163</th>\n",
       "      <td>442704</td>\n",
       "      <td>440298</td>\n",
       "      <td>How do I delete a question from Quora?</td>\n",
       "      <td>Can I delete all the questions I asked on Quora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20164</th>\n",
       "      <td>301151</td>\n",
       "      <td>354200</td>\n",
       "      <td>Why is Hillary Clinton a better choice than Do...</td>\n",
       "      <td>Is Hillary Clinton really worse than Donald Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20165</th>\n",
       "      <td>28097</td>\n",
       "      <td>377142</td>\n",
       "      <td>How does 1 Billion Rising stop violence agains...</td>\n",
       "      <td>When will Indians learn to respect women and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20166</th>\n",
       "      <td>256441</td>\n",
       "      <td>265093</td>\n",
       "      <td>What is the scope after physics honours?</td>\n",
       "      <td>What is the scope after bsc physics?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20167</th>\n",
       "      <td>368041</td>\n",
       "      <td>13426</td>\n",
       "      <td>Why do so many people hate nickelback?</td>\n",
       "      <td>Why don't people like Nickelback?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20168</th>\n",
       "      <td>92752</td>\n",
       "      <td>310096</td>\n",
       "      <td>What skills are needed for machine learning jobs?</td>\n",
       "      <td>What other skills do you need as a programmer?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20169</th>\n",
       "      <td>115691</td>\n",
       "      <td>109133</td>\n",
       "      <td>How can you tell if a guy likes you or not?</td>\n",
       "      <td>How can you tell a if guy likes you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170</th>\n",
       "      <td>317191</td>\n",
       "      <td>392872</td>\n",
       "      <td>What is the best rap song ever made?</td>\n",
       "      <td>What are the best rap songs of all time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171</th>\n",
       "      <td>198686</td>\n",
       "      <td>61805</td>\n",
       "      <td>What is the best method of learning to speak a...</td>\n",
       "      <td>What is the best method to learn new language?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20172</th>\n",
       "      <td>264474</td>\n",
       "      <td>122621</td>\n",
       "      <td>Is PM Narendra Modi the brand ambassador of Re...</td>\n",
       "      <td>Why did Narendra Modi allow Reliance to publis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20173</th>\n",
       "      <td>344052</td>\n",
       "      <td>129175</td>\n",
       "      <td>How does one become a video game designer or d...</td>\n",
       "      <td>What should we do to become a game designer?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20174</th>\n",
       "      <td>407592</td>\n",
       "      <td>178407</td>\n",
       "      <td>Montana State Football Live Stream | Watch Mon...</td>\n",
       "      <td>Idaho State Football Live Stream | Watch Idaho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20175</th>\n",
       "      <td>394268</td>\n",
       "      <td>56248</td>\n",
       "      <td>Why nobody answer my questions in Quora?</td>\n",
       "      <td>Why do people never answer my question on Quora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20176</th>\n",
       "      <td>146511</td>\n",
       "      <td>200961</td>\n",
       "      <td>What is it like to have a huge penis?</td>\n",
       "      <td>Whats it like to have a huge penis?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20177</th>\n",
       "      <td>37354</td>\n",
       "      <td>401247</td>\n",
       "      <td>How much does each miner in Gold Rush: Alaska ...</td>\n",
       "      <td>What is the biggest gold nugget found on a TV ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20178</th>\n",
       "      <td>282027</td>\n",
       "      <td>425313</td>\n",
       "      <td>What are all the places that I can visit in Ch...</td>\n",
       "      <td>What are the places to visit in Chennai?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20179 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid2  question1                                          question2  \\\n",
       "qid1                                                                          \n",
       "0      245776       2705    What are the best sites to book a hotel online?   \n",
       "1      104796      48346                       How can I stop masturbation?   \n",
       "2       41770     383018            Which is the best way to control anger?   \n",
       "3       81132     401393  Why is my Miniature Pinscher/Chihuahua mix afr...   \n",
       "4      244572       7520          How do I get rid off from porn addiction?   \n",
       "5      436197     287812      What is the easiest way to learn how to draw?   \n",
       "6       61578     451982         What secret can you not share with anyone?   \n",
       "7      134731     120381                   How can I improve my vocabulary?   \n",
       "8      383155      74916  How do I calculate square roots and cubed root...   \n",
       "9      274533     376894  What are the best books to read to learn about...   \n",
       "10     269621     440889  How can I stop spending so much time on the In...   \n",
       "11     448829      46013  What is a good free editing software for PC ga...   \n",
       "12     450096     318304           Why is India performing bad in Olympics?   \n",
       "13     392910     124619  What was the mystery behind Padmanabha Swamy T...   \n",
       "14     372844     159742  Why is Saltwater Taffy candy imported in Portu...   \n",
       "15      44306     266477    What programming language I should learn first?   \n",
       "16     214806     256610           How does one ask a question anonymously?   \n",
       "17      26659     218948                   How imminent is World War three?   \n",
       "18      18431     126901  What are your views on ban of 500 and 1000 rup...   \n",
       "19      97343     387016   Which is the best photo editing app for android?   \n",
       "20      57110     392060  How can one meet British/Irish people in the T...   \n",
       "21     441670     212168  What is the most badass thing anyone has ever ...   \n",
       "22     288752     244179                         Do animals commit suicide?   \n",
       "23      26518      62322   In which city in Kerala is famous for honeymoon?   \n",
       "24     300871     188843                     How can I watch movies online?   \n",
       "25     378990     263801  Can you get pregnant the day before your perio...   \n",
       "26     303408     370315          How will long distance relationship work?   \n",
       "27     187628     399080  Would a Donald Trump presidency really be so bad?   \n",
       "28     335928      21635  Justin Bieber (musician): Justinbieber what ar...   \n",
       "29     150088      14346  Can a girl get pregnant after her last day of ...   \n",
       "...       ...        ...                                                ...   \n",
       "20149  103125     409631                 How can I get started using Quora?   \n",
       "20150  314384      65383    How can I convert audible AAX files to MP3 mac?   \n",
       "20151  203865     223489     Does sure gell help pass a drug test for meth?   \n",
       "20152   43855     340063          Does massage really increase breast size?   \n",
       "20153  353886     205442        How can one track a mobile number location?   \n",
       "20154  295838     286664                     How did you quit/stop smoking?   \n",
       "20155   58482     127486  I want to live the rest of my life alone and w...   \n",
       "20156  451324     113225                  How did Trump win America's vote?   \n",
       "20157   46094     347874  What is the biggest mistake people do when the...   \n",
       "20158  367645     287563  What do most Americans (from the US) think of ...   \n",
       "20159  232757     358272  What is the craziest question ever asked on Qu...   \n",
       "20160  274892     122148           How can I improve my realistic drawings?   \n",
       "20161  411133     341720  What are some of the best Hollywood movies to ...   \n",
       "20162  118359     110608  Self-Improvement: How can I motivate myself to...   \n",
       "20163  442704     440298             How do I delete a question from Quora?   \n",
       "20164  301151     354200  Why is Hillary Clinton a better choice than Do...   \n",
       "20165   28097     377142  How does 1 Billion Rising stop violence agains...   \n",
       "20166  256441     265093           What is the scope after physics honours?   \n",
       "20167  368041      13426             Why do so many people hate nickelback?   \n",
       "20168   92752     310096  What skills are needed for machine learning jobs?   \n",
       "20169  115691     109133        How can you tell if a guy likes you or not?   \n",
       "20170  317191     392872               What is the best rap song ever made?   \n",
       "20171  198686      61805  What is the best method of learning to speak a...   \n",
       "20172  264474     122621  Is PM Narendra Modi the brand ambassador of Re...   \n",
       "20173  344052     129175  How does one become a video game designer or d...   \n",
       "20174  407592     178407  Montana State Football Live Stream | Watch Mon...   \n",
       "20175  394268      56248           Why nobody answer my questions in Quora?   \n",
       "20176  146511     200961              What is it like to have a huge penis?   \n",
       "20177   37354     401247  How much does each miner in Gold Rush: Alaska ...   \n",
       "20178  282027     425313  What are all the places that I can visit in Ch...   \n",
       "\n",
       "                                            is_duplicate  \n",
       "qid1                                                      \n",
       "0                What is the best hotel booking service?  \n",
       "1                     How can I stop doing masturbation?  \n",
       "2            What is the best way to control your anger?  \n",
       "3      Why is my Black Lab/Pitbull mix puppy afraid o...  \n",
       "4      What is the best way to overcome an porn addic...  \n",
       "5                            How do I learn how to draw?  \n",
       "6      What is the one secret you can never share wit...  \n",
       "7      What are some good ways to improve English voc...  \n",
       "8      What is the method to calculate a square root ...  \n",
       "9                  Which Is the best book on psychology?  \n",
       "10        How do I stop wasting my time on the internet?  \n",
       "11             Which is the best video editing software?  \n",
       "12     What is the reason of the poor performance of ...  \n",
       "13     What is the real mystery behind Padmanabhaswam...  \n",
       "14     Why is Saltwater taffy candy imported in South...  \n",
       "15     What is the best programming language I should...  \n",
       "16       How do I answer questions anonymously on Quora?  \n",
       "17           Is World War 3 more imminent than expected?  \n",
       "18     What are your views on banning 500 and 1000 ru...  \n",
       "19     What is the best photo editing app for Android...  \n",
       "20              How can I meet British people in Canada?  \n",
       "21     What is the most badass thing you have ever do...  \n",
       "22              Can animals think of committing suicide?  \n",
       "23     Which is the best resort for honeymoon in kerala?  \n",
       "24             Where can I watch movies online for free?  \n",
       "25     If I have a 6 day period can I get pregnant ju...  \n",
       "26     What does it take for a successful long distan...  \n",
       "27     What would America look like with Donald Trump...  \n",
       "28                 Can you see who views your Instagram?  \n",
       "29     Can I get pregnant 3 days after period if I bl...  \n",
       "...                                                  ...  \n",
       "20149  How do I get started using Quora? Why all my w...  \n",
       "20150        How can I convert audible AAX files to MP3?  \n",
       "20151  Can hair dye help pass a hair follicle drug te...  \n",
       "20152  Do breast massages really help in increasing b...  \n",
       "20153  How can I track a mobile number and the locati...  \n",
       "20154                            How can I stop smoking?  \n",
       "20155  How do I accept that I will live alone the res...  \n",
       "20156             How did Donald Trump win the election?  \n",
       "20157  What are the biggest mistakes people make when...  \n",
       "20158    What do American girls think about Indian guys?  \n",
       "20159           What is the strangest question on Quora?  \n",
       "20160                     How can I improve my drawing ?  \n",
       "20161        Which is the all time best Hollywood movie?  \n",
       "20162             Why do I lack motivation to work hard?  \n",
       "20163   Can I delete all the questions I asked on Quora?  \n",
       "20164  Is Hillary Clinton really worse than Donald Tr...  \n",
       "20165  When will Indians learn to respect women and s...  \n",
       "20166               What is the scope after bsc physics?  \n",
       "20167                  Why don't people like Nickelback?  \n",
       "20168     What other skills do you need as a programmer?  \n",
       "20169               How can you tell a if guy likes you?  \n",
       "20170           What are the best rap songs of all time?  \n",
       "20171     What is the best method to learn new language?  \n",
       "20172  Why did Narendra Modi allow Reliance to publis...  \n",
       "20173       What should we do to become a game designer?  \n",
       "20174  Idaho State Football Live Stream | Watch Idaho...  \n",
       "20175   Why do people never answer my question on Quora?  \n",
       "20176                Whats it like to have a huge penis?  \n",
       "20177  What is the biggest gold nugget found on a TV ...  \n",
       "20178           What are the places to visit in Chennai?  \n",
       "\n",
       "[20179 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.read_csv('data/train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv('data/test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "data_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "Y = data_train.pop(\"is_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_train\n",
    "X_test = features_test\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(value=0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X\n",
    "#data.pop(\"question1\")\n",
    "#data.pop(\"question2\")\n",
    "#data.pop(\"is_duplicate\")\n",
    "X_train= features_train.drop(['question1','question2','is_duplicate','cosine_distance','jaccard_distance','euclidean_distance','norm_wmd','fuzz_WRatio','len_word_q2','len_word_q1','minkowski_distance','braycurtis_distance'], axis=1)\n",
    "test_size = 0.1\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, Y, test_size=test_size)\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/10\n",
      "72090/72090 [==============================] - 273s 4ms/step - loss: 0.5719 - val_loss: 0.5572\n",
      "Epoch 2/10\n",
      "72090/72090 [==============================] - 162s 2ms/step - loss: 0.5489 - val_loss: 0.5507\n",
      "Epoch 3/10\n",
      "72090/72090 [==============================] - 157s 2ms/step - loss: 0.5407 - val_loss: 0.5414\n",
      "Epoch 4/10\n",
      "72090/72090 [==============================] - 157s 2ms/step - loss: 0.5368 - val_loss: 0.5550\n",
      "Epoch 5/10\n",
      "72090/72090 [==============================] - 158s 2ms/step - loss: 0.5318 - val_loss: 0.5314\n",
      "Epoch 6/10\n",
      "72090/72090 [==============================] - 169s 2ms/step - loss: 0.5290 - val_loss: 0.5328\n",
      "Epoch 7/10\n",
      "72090/72090 [==============================] - 288s 4ms/step - loss: 0.5247 - val_loss: 0.5434\n",
      "Epoch 8/10\n",
      "72090/72090 [==============================] - 358s 5ms/step - loss: 0.5230 - val_loss: 0.5263\n",
      "Epoch 9/10\n",
      "72090/72090 [==============================] - 414s 6ms/step - loss: 0.5204 - val_loss: 0.5331\n",
      "Epoch 10/10\n",
      "72090/72090 [==============================] - 322s 4ms/step - loss: 0.5190 - val_loss: 0.5454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11357bbd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "X_train_np = X_train.values\n",
    "X_val_np = X_val.values\n",
    "Y_train_np = Y_train.values\n",
    "Y_val_np  = Y_val.values\n",
    "\n",
    "shape = (X_train_np.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='sigmoid',input_shape=shape))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "#model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "model.fit(X_train_np,Y_train_np,nb_epoch=10,validation_data =(X_val_np,Y_val_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzz_qratio</th>\n",
       "      <th>fuzz_WRatio</th>\n",
       "      <th>...</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>skew_q1vec</th>\n",
       "      <th>skew_q2vec</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61470</th>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.397956</td>\n",
       "      <td>-5.397956</td>\n",
       "      <td>50.999782</td>\n",
       "      <td>50.999782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52652</th>\n",
       "      <td>115</td>\n",
       "      <td>39</td>\n",
       "      <td>76</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>6.248636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.564608</td>\n",
       "      <td>0.454233</td>\n",
       "      <td>0.206826</td>\n",
       "      <td>0.378103</td>\n",
       "      <td>-7.113229</td>\n",
       "      <td>-4.746818</td>\n",
       "      <td>65.892186</td>\n",
       "      <td>38.016010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62647</th>\n",
       "      <td>104</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>6.700246</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.466555</td>\n",
       "      <td>0.477150</td>\n",
       "      <td>0.213258</td>\n",
       "      <td>0.390585</td>\n",
       "      <td>-5.871459</td>\n",
       "      <td>-4.239369</td>\n",
       "      <td>46.448806</td>\n",
       "      <td>28.991935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>-4</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>11.052061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>166.510503</td>\n",
       "      <td>0.815223</td>\n",
       "      <td>0.377213</td>\n",
       "      <td>0.485439</td>\n",
       "      <td>-0.580091</td>\n",
       "      <td>-2.302947</td>\n",
       "      <td>1.765170</td>\n",
       "      <td>14.246697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>8.190089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153.197744</td>\n",
       "      <td>0.605530</td>\n",
       "      <td>0.279310</td>\n",
       "      <td>0.394413</td>\n",
       "      <td>-3.854237</td>\n",
       "      <td>-2.359391</td>\n",
       "      <td>31.850361</td>\n",
       "      <td>19.318841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       len_q1  len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  \\\n",
       "61470      30      26         4           15           13            6   \n",
       "52652     115      39        76           23           16           25   \n",
       "62647     104      99         5           23           27           16   \n",
       "605        38      42        -4           15           16            7   \n",
       "5464       43      35         8           20           16            8   \n",
       "\n",
       "       len_word_q2  common_words  fuzz_qratio  fuzz_WRatio    ...      \\\n",
       "61470            5             2           67           70    ...       \n",
       "52652            9             6           38           86    ...       \n",
       "62647           16             6           62           62    ...       \n",
       "605              7             3           79           79    ...       \n",
       "5464             7             2           58           58    ...       \n",
       "\n",
       "       cityblock_distance  jaccard_distance  canberra_distance  \\\n",
       "61470            0.000000               0.0           0.000000   \n",
       "52652            6.248636               1.0         157.564608   \n",
       "62647            6.700246               1.0         160.466555   \n",
       "605             11.052061               1.0         166.510503   \n",
       "5464             8.190089               1.0         153.197744   \n",
       "\n",
       "       euclidean_distance  minkowski_distance  braycurtis_distance  \\\n",
       "61470            0.000000            0.000000             0.000000   \n",
       "52652            0.454233            0.206826             0.378103   \n",
       "62647            0.477150            0.213258             0.390585   \n",
       "605              0.815223            0.377213             0.485439   \n",
       "5464             0.605530            0.279310             0.394413   \n",
       "\n",
       "       skew_q1vec  skew_q2vec  kur_q1vec  kur_q2vec  \n",
       "61470   -5.397956   -5.397956  50.999782  50.999782  \n",
       "52652   -7.113229   -4.746818  65.892186  38.016010  \n",
       "62647   -5.871459   -4.239369  46.448806  28.991935  \n",
       "605     -0.580091   -2.302947   1.765170  14.246697  \n",
       "5464    -3.854237   -2.359391  31.850361  19.318841  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>...</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>skew_q1vec</th>\n",
       "      <th>skew_q2vec</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19866</th>\n",
       "      <td>What is a good way to spend a long weekend in ...</td>\n",
       "      <td>What is the best way to spend a weekend in Ban...</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>3.778034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.590874</td>\n",
       "      <td>0.273035</td>\n",
       "      <td>0.124839</td>\n",
       "      <td>0.236917</td>\n",
       "      <td>-6.923558</td>\n",
       "      <td>-6.190262</td>\n",
       "      <td>63.793805</td>\n",
       "      <td>53.960178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38013</th>\n",
       "      <td>What does it mean if a dog vomits white foam?</td>\n",
       "      <td>What does it mean if a dog is throwing up yell...</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>77</td>\n",
       "      <td>-32</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>6.219466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.157158</td>\n",
       "      <td>0.445947</td>\n",
       "      <td>0.200436</td>\n",
       "      <td>0.341283</td>\n",
       "      <td>-3.553175</td>\n",
       "      <td>-4.978099</td>\n",
       "      <td>25.958704</td>\n",
       "      <td>42.166227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60509</th>\n",
       "      <td>How can you make your skin lighter?</td>\n",
       "      <td>What can I do to make my skin whiter?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>-2</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>6.720695</td>\n",
       "      <td>1.0</td>\n",
       "      <td>137.431993</td>\n",
       "      <td>0.473985</td>\n",
       "      <td>0.210055</td>\n",
       "      <td>0.325232</td>\n",
       "      <td>-4.250795</td>\n",
       "      <td>-2.831428</td>\n",
       "      <td>34.080928</td>\n",
       "      <td>20.905081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73053</th>\n",
       "      <td>What is the best passive investment strategy?</td>\n",
       "      <td>What is your investment strategy?</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.982412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>137.046347</td>\n",
       "      <td>0.435726</td>\n",
       "      <td>0.200231</td>\n",
       "      <td>0.304307</td>\n",
       "      <td>-5.354166</td>\n",
       "      <td>-3.369078</td>\n",
       "      <td>46.089431</td>\n",
       "      <td>23.742220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56738</th>\n",
       "      <td>What is the best way to start learning program...</td>\n",
       "      <td>How do I start learning programming language? ...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>4.675204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.059820</td>\n",
       "      <td>0.340347</td>\n",
       "      <td>0.155126</td>\n",
       "      <td>0.297584</td>\n",
       "      <td>-6.921454</td>\n",
       "      <td>-6.508542</td>\n",
       "      <td>65.565907</td>\n",
       "      <td>58.963952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question1  \\\n",
       "19866  What is a good way to spend a long weekend in ...   \n",
       "38013      What does it mean if a dog vomits white foam?   \n",
       "60509                How can you make your skin lighter?   \n",
       "73053      What is the best passive investment strategy?   \n",
       "56738  What is the best way to start learning program...   \n",
       "\n",
       "                                               question2  is_duplicate  \\\n",
       "19866  What is the best way to spend a weekend in Ban...             1   \n",
       "38013  What does it mean if a dog is throwing up yell...             1   \n",
       "60509              What can I do to make my skin whiter?             1   \n",
       "73053                  What is your investment strategy?             0   \n",
       "56738  How do I start learning programming language? ...             1   \n",
       "\n",
       "       len_q1  len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  \\\n",
       "19866      67      53        14           20           20           14   \n",
       "38013      45      77       -32           16           23           10   \n",
       "60509      35      37        -2           19           18            7   \n",
       "73053      45      33        12           16           16            7   \n",
       "56738      71      70         1           21           21           12   \n",
       "\n",
       "       len_word_q2    ...      cityblock_distance  jaccard_distance  \\\n",
       "19866           11    ...                3.778034               1.0   \n",
       "38013           17    ...                6.219466               1.0   \n",
       "60509            9    ...                6.720695               1.0   \n",
       "73053            5    ...                5.982412               1.0   \n",
       "56738           12    ...                4.675204               1.0   \n",
       "\n",
       "       canberra_distance  euclidean_distance  minkowski_distance  \\\n",
       "19866         128.590874            0.273035            0.124839   \n",
       "38013         147.157158            0.445947            0.200436   \n",
       "60509         137.431993            0.473985            0.210055   \n",
       "73053         137.046347            0.435726            0.200231   \n",
       "56738         143.059820            0.340347            0.155126   \n",
       "\n",
       "       braycurtis_distance  skew_q1vec  skew_q2vec  kur_q1vec  kur_q2vec  \n",
       "19866             0.236917   -6.923558   -6.190262  63.793805  53.960178  \n",
       "38013             0.341283   -3.553175   -4.978099  25.958704  42.166227  \n",
       "60509             0.325232   -4.250795   -2.831428  34.080928  20.905081  \n",
       "73053             0.304307   -5.354166   -3.369078  46.089431  23.742220  \n",
       "56738             0.297584   -6.921454   -6.508542  65.565907  58.963952  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
