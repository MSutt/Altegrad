{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMD Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'C:/Users/Amine/Desktop/MVA2017/Object_recognition/Project/project/Project/KCCA/glove.42B.300d.txt.word2vec'\n",
    "word_embedding_model_glove = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Amine/Desktop/MVA2017/ALTEGRAD/TP3/for moodle/code/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features for train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some useful features:\n",
    "\n",
    "    -Using the fuzz package to compute some ratio of string similarity between question 1 et question 2.\n",
    "    -The length of questions\n",
    "    -Word embedding of the questions and compute different distances: WMD, cosine, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "data_train = data_train.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding of the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=word_embedding_model_glove\n",
    "norm_model = word_embedding_model_glove\n",
    "norm_model.init_sims(replace=True)\n",
    "\n",
    "data_train['wmd'] = data_train.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data_train.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data_train.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data_train.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data_train.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_train['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data_train['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_train['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(question1_vectors, open('data/q1_glove.pkl', 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open('data/q2_glove.pkl', 'wb'), -1)\n",
    "\n",
    "data_train.to_csv('data/train_features_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generete features for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['len_q1'] = data_test.question1.apply(lambda x: len(str(x)))\n",
    "data_test['len_q2'] = data_test.question2.apply(lambda x: len(str(x)))\n",
    "data_test['diff_len'] = data_test.len_q1 - data_test.len_q2\n",
    "data_test['len_char_q1'] = data_test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_test['len_char_q2'] = data_test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_test['len_word_q1'] = data_test.question1.apply(lambda x: len(str(x).split()))\n",
    "data_test['len_word_q2'] = data_test.question2.apply(lambda x: len(str(x).split()))\n",
    "data_test['common_words'] = data_test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_test['fuzz_qratio'] = data_test.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_WRatio'] = data_test.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_ratio'] = data_test.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_token_set_ratio'] = data_test.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_token_sort_ratio'] = data_test.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_token_set_ratio'] = data_test.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_token_sort_ratio'] = data_test.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_model = model\n",
    "norm_model.init_sims(replace=True)\n",
    "data_test['wmd'] = data_test.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data_test.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data_test.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data_test.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data_test.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_test['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data_test['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_test['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(question1_vectors, open('data/q1_glove_test.pkl', 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open('data/q2_glove_test.pkl', 'wb'), -1)\n",
    "data_test.to_csv('data/test_features_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = pd.read_csv('data/train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv('data/test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "data_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "Y_train=data_train[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/zfturbo/pagerank-on-quora-feature-file-generator/code\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import gc \n",
    "\n",
    "df_train = pd.read_csv('data/train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "df_test = pd.read_csv('data/test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "# Generating a graph of Questions and their neighbors\n",
    "def generate_qid_graph_table(row):\n",
    "    hash_key1 = hashlib.md5(row[\"question1\"].encode('utf-8')).hexdigest()\n",
    "    hash_key2 = hashlib.md5(row[\"question2\"].encode('utf-8')).hexdigest()\n",
    "\n",
    "    qid_graph.setdefault(hash_key1, []).append(hash_key2)\n",
    "    qid_graph.setdefault(hash_key2, []).append(hash_key1)\n",
    "\n",
    "\n",
    "qid_graph = {}\n",
    "print('Apply to train...')\n",
    "df_train.apply(generate_qid_graph_table, axis=1)\n",
    "print('Apply to test...')\n",
    "df_test.apply(generate_qid_graph_table, axis=1)\n",
    "\n",
    "\n",
    "def pagerank():\n",
    "    MAX_ITER = 20\n",
    "    d = 0.85\n",
    "\n",
    "    # Initializing -- every node gets a uniform value!\n",
    "    pagerank_dict = {i: 1 / len(qid_graph) for i in qid_graph}\n",
    "    num_nodes = len(pagerank_dict)\n",
    "\n",
    "    for iter in range(0, MAX_ITER):\n",
    "\n",
    "        for node in qid_graph:\n",
    "            local_pr = 0\n",
    "\n",
    "            for neighbor in qid_graph[node]:\n",
    "                local_pr += pagerank_dict[neighbor] / len(qid_graph[neighbor])\n",
    "\n",
    "            pagerank_dict[node] = (1 - d) / num_nodes + d * local_pr\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "print('Main PR generator...')\n",
    "pagerank_dict = pagerank()\n",
    "\n",
    "def get_pagerank_value(row):\n",
    "    q1 = hashlib.md5(row[\"question1\"].encode('utf-8')).hexdigest()\n",
    "    q2 = hashlib.md5(row[\"question2\"].encode('utf-8')).hexdigest()\n",
    "    s = pd.Series({\n",
    "        \"q1_pr\": pagerank_dict[q1],\n",
    "        \"q2_pr\": pagerank_dict[q2]\n",
    "    })\n",
    "    return s\n",
    "\n",
    "print('Apply to train...')\n",
    "pagerank_feats_train = df_train.apply(get_pagerank_value, axis=1)\n",
    "print('Writing train...')\n",
    "pagerank_feats_train.to_csv(\"data/pagerank_train.csv\", index=False)\n",
    "del df_train\n",
    "gc.collect()\n",
    "print('Apply to test...')\n",
    "pagerank_feats_test = df_test.apply(get_pagerank_value, axis=1)\n",
    "print('Writing test...')\n",
    "pagerank_feats_test.to_csv(\"data/pagerank_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features of page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test[\"q1_pr\"]=pagerank_feats_test[\"q1_pr\"]\n",
    "features_test[\"q2_pr\"]=pagerank_feats_test[\"q2_pr\"]\n",
    "features_train[\"q1_pr\"]=pagerank_feats_train[\"q1_pr\"]\n",
    "features_train[\"q2_pr\"]=pagerank_feats_train[\"q2_pr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has the purpose to compute the fraquency of the questions with the idea that more frequenct questions are more likely to be duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code from https://www.kaggle.com/jturkewitz/magic-features-0-03-gain/comments\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "train_orig =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "test_orig =  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "tic0=timeit.default_timer()\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df1_test = test_orig[['question1']].copy()\n",
    "df2_test = test_orig[['question2']].copy()\n",
    "\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "train_cp = train_orig.copy()\n",
    "test_cp = test_orig.copy()\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "# print(comb['q1_hash'])\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "corr_mat = train_comb.corr()\n",
    "train_comb.to_csv('./data/train_magic.csv', columns=['q1_freq', 'q2_freq'])\n",
    "test_comb.to_csv('./data/test_magic.csv', columns=['q1_freq', 'q2_freq'])\n",
    "print(corr_mat)\n",
    "print(test_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "train_orig =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "test_orig =  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "ques = pd.concat([train_orig[['question1', 'question2']], test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "\n",
    "\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "    q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "    q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "train_orig['q1_q2_intersect'] = train_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_orig['q1_q2_intersect'] = test_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "\n",
    "train_feat = train_orig[['q1_q2_intersect']]\n",
    "test_feat = test_orig[['q1_q2_intersect']]\n",
    "\n",
    "train_feat.to_csv('./data/train_magic_v2.csv')\n",
    "test_feat.to_csv('./data/test_magic_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import dist_utils,split_data\n",
    "import networkx as nx\n",
    "import scipy.stats as sps\n",
    "\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "\n",
    "train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"label\"])\n",
    "test = pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "data_all = pd.concat([train, test])[['question1','question2']]\n",
    "\n",
    "#dup index\n",
    "q_all = pd.DataFrame(np.hstack([train['question1'], test['question1'],\n",
    "                   train['question2'], test['question2']]), columns=['question'])\n",
    "q_all = pd.DataFrame(q_all.question.value_counts()).reset_index()\n",
    "\n",
    "q_num = dict(q_all.values)\n",
    "q_index = {}\n",
    "for i,key in enumerate(q_num.keys()):\n",
    "    q_index[key] = i\n",
    "data_all['q1_index'] = data_all['question1'].map(q_index)\n",
    "data_all['q2_index'] = data_all['question2'].map(q_index)\n",
    "\n",
    "\n",
    "#link edges\n",
    "q_list = {}\n",
    "dd = data_all[['q1_index','q2_index']].values\n",
    "for i in tqdm(np.arange(data_all.shape[0])):\n",
    "#for i in np.arange(dd.shape[0]):\n",
    "    q1,q2=dd[i]\n",
    "    if q_list.setdefault(q1,[q2])!=[q2]:\n",
    "        q_list[q1].append(q2)\n",
    "    if q_list.setdefault(q2,[q1])!=[q1]:\n",
    "        q_list[q2].append(q1)\n",
    "\n",
    "\n",
    "common_fea = np.zeros((data_all.shape[0],3))\n",
    "for i in tqdm(np.arange(data_all.shape[0])):\n",
    "    q1,q2 = dd[i]\n",
    "    if (q1 not in q_list)|(q2 not in q_list):\n",
    "        continue\n",
    "    nei_q1 = set(q_list[q1])\n",
    "    nei_q2 = set(q_list[q2])\n",
    "\n",
    "    f_1 = len(nei_q1.intersection(nei_q2))\n",
    "    common_fea[i][0] = f_1\n",
    "    common_fea[i][1] = len(nei_q1)\n",
    "    common_fea[i][2] = len(nei_q2)\n",
    "\n",
    "train_common = common_fea[:train.shape[0]]\n",
    "test_common = common_fea[train.shape[0]:]\n",
    "\n",
    "# pd.to_pickle(train_common,'data/train_neigh.pkl')\n",
    "# pd.to_pickle(test_common,'data/test_neigh.pkl')\n",
    "train_cores=pd.DataFrame(train_common, columns=['core1','core2','core3'])\n",
    "test_cores=pd.DataFrame(test_common, columns=['core1','core2','core3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"label\"])\n",
    "df_test = pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "dfs = (df_train, df_test)\n",
    "\n",
    "questions = []\n",
    "for df in dfs:\n",
    "    df['question1'] = df['question1'].str.lower()\n",
    "    df['question2'] = df['question2'].str.lower()\n",
    "    questions += df['question1'].tolist()\n",
    "    questions += df['question2'].tolist()\n",
    "\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(questions)\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    edges = list(df[['question1', 'question2']].to_records(index=False))\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "graph.remove_edges_from(graph.selfloop_edges())\n",
    "\n",
    "df = pd.DataFrame(data=graph.nodes(), columns=[\"question\"])\n",
    "df['kcores'] = 1\n",
    "\n",
    "n_cores = 30\n",
    "for k in tqdm(range(2, n_cores + 1)):\n",
    "    ck = nx.k_core(graph, k=k).nodes()\n",
    "    df['kcores'][df.question.isin(ck)] = k\n",
    "\n",
    "print(df['kcores'].value_counts())\n",
    "\n",
    "df.to_csv(\"data/question_kcores.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"label\"])\n",
    "df_test = pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "dfs = (df_train, df_test)\n",
    "\n",
    "for df in dfs:\n",
    "    df['question1'] = df['question1'].str.lower()\n",
    "    df['question2'] = df['question2'].str.lower()\n",
    "    \n",
    "    q_kcores = pd.read_csv('data/question_kcores.csv', encoding=\"ISO-8859-1\")\n",
    "    \n",
    "    q_kcores['question1'] = q_kcores['question']\n",
    "    del q_kcores['question']\n",
    "    df['q1_kcores'] = df.merge(q_kcores, 'left')['kcores']\n",
    "    \n",
    "    q_kcores['question2'] = q_kcores['question1']\n",
    "    del q_kcores['question1']\n",
    "    df['q2_kcores'] = df.merge(q_kcores, 'left')['kcores']\n",
    "    \n",
    "    df['q1_q2_kcores_ratio'] = (df['q1_kcores'] / df['q2_kcores']).apply(lambda x: x if x < 1. else 1./x)\n",
    "    df['q1_q2_kcores_diff'] = (df['q1_kcores'] - df['q2_kcores']).apply(abs)\n",
    "    df['q1_q2_kcores_diff_normed'] = (df['q1_kcores'] - df['q2_kcores']).apply(abs) / (df['q1_kcores'] + df['q2_kcores'])\n",
    "\n",
    "df_train, df_test = dfs\n",
    "df_train = df_train[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]\n",
    "df_test = df_test[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def word_match_share(row, stops=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(row):\n",
    "    return len(set(row['question1']).intersection(set(row['question2'])))\n",
    "\n",
    "def total_unique_words(row):\n",
    "    return len(set(row['question1']).union(row['question2']))\n",
    "\n",
    "def total_unq_words_stop(row, stops):\n",
    "    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n",
    "\n",
    "def wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "def wc_ratio(row):\n",
    "    l1 = len(row['question1'])*1.0 \n",
    "    l2 = len(row['question2'])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "def wc_ratio_unique(row):\n",
    "    l1 = len(set(row['question1'])) * 1.0\n",
    "    l2 = len(set(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(row, stops=None):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(row, stops=None):\n",
    "    l1 = len([x for x in set(row['question1']) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(row['question2']) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(row):\n",
    "    if not row['question1'] or not row['question2']:\n",
    "        return np.nan\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "def char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "def char_ratio(row):\n",
    "    l1 = len(''.join(row['question1'])) \n",
    "    l2 = len(''.join(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(row, stops=None):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n",
    "\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def tfidf_word_match_share_stops(row, stops=None, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "\n",
    "def build_features(data, stops, weights):\n",
    "    X = pd.DataFrame()\n",
    "    f = functools.partial(word_match_share, stops=stops)\n",
    "    X['word_match'] = data.apply(f, axis=1, raw=True) #1\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "    X['tfidf_wm'] = data.apply(f, axis=1, raw=True) #2\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\n",
    "    X['tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\n",
    "\n",
    "    X['jaccard'] = data.apply(jaccard, axis=1, raw=True) #4\n",
    "    X['wc_diff'] = data.apply(wc_diff, axis=1, raw=True) #5\n",
    "    X['wc_ratio'] = data.apply(wc_ratio, axis=1, raw=True) #6\n",
    "    X['wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True) #7\n",
    "    X['wc_ratio_unique'] = data.apply(wc_ratio_unique, axis=1, raw=True) #8\n",
    "\n",
    "    f = functools.partial(wc_diff_unique_stop, stops=stops)    \n",
    "    X['wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\n",
    "    f = functools.partial(wc_ratio_unique_stop, stops=stops)    \n",
    "    X['wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\n",
    "\n",
    "    X['same_start'] = data.apply(same_start_word, axis=1, raw=True) #11\n",
    "    X['char_diff'] = data.apply(char_diff, axis=1, raw=True) #12\n",
    "\n",
    "    f = functools.partial(char_diff_unique_stop, stops=stops) \n",
    "    X['char_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #13\n",
    "\n",
    "#     X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\n",
    "    X['total_unique_words'] = data.apply(total_unique_words, axis=1, raw=True)  #15\n",
    "\n",
    "    f = functools.partial(total_unq_words_stop, stops=stops)\n",
    "    X['total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\n",
    "    \n",
    "    X['char_ratio'] = data.apply(char_ratio, axis=1, raw=True) #17    \n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_train =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "df_train = df_train.fillna(' ')\n",
    "\n",
    "df_test=  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "ques = pd.concat([df_train[['question1', 'question2']], \\\n",
    "    df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "test_leaky = df_test.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "del df_test\n",
    "\n",
    "train_leaky = df_train.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "\n",
    "# explore\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n",
    "\n",
    "words = [x for y in train_qs for x in y]\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "print('Building Features')\n",
    "x_train = build_features(df_train, stops, weights)\n",
    "x_train = pd.concat((x_train, train_leaky), axis=1)\n",
    "\n",
    "df_test =pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "df_test = df_test.fillna(' ')\n",
    "\n",
    "df_test['question1'] = df_test['question1'].map(lambda x: str(x).lower().split())\n",
    "df_test['question2'] = df_test['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "x_test = build_features(df_test, stops, weights)\n",
    "x_test = pd.concat((x_test, test_leaky), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_naive_features(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    df['q1_char_length_with_space'] = df.question1.apply(lambda x: len(str(x)))  # with space\n",
    "    df['q1_char_length_without_space'] = df.question1.apply(lambda x: len(str(x).replace(' ', '')))  # without space\n",
    "    df['q1_word_length'] = df.question1.apply(lambda x: len(str(x).split(' ')))\n",
    "    df['q1_question_mark'] = df.question1.apply(lambda x: str(x).count('?'))  # TODO: prob not good?\n",
    "\n",
    "    df['q2_char_length_with_space'] = df.question2.apply(lambda x: len(str(x)))  # with space\n",
    "    df['q2_char_length_without_space'] = df.question2.apply(lambda x: len(str(x).replace(' ', '')))  # without space\n",
    "    df['q2_word_length'] = df.question2.apply(lambda x: len(str(x).split(' ')))\n",
    "    df['q2_question_mark'] = df.question2.apply(lambda x: str(x).count('?'))  # TODO: prob not good?\n",
    "\n",
    "    df['word_length_diff'] = abs(df.q2_word_length - df.q1_word_length)\n",
    "    df['char_length_diff'] = abs(df.q2_char_length_without_space - df.q1_char_length_without_space)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_data =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "train_data = train_data.fillna(' ')\n",
    "\n",
    "test_data=  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "\n",
    "train_data = generate_naive_features(train_data)\n",
    "test_data = generate_naive_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
    "    \n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    return 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "def word_shares(row):\n",
    "\n",
    "\tq1_list = str(row['question1']).lower().split()\n",
    "\tq1 = set(q1_list)\n",
    "\tq1words = q1.difference(stops)\n",
    "\tif len(q1words) == 0:\n",
    "\t\treturn '0:0:0:0:0:0:0:0'\n",
    "        \n",
    "\tq2_list = str(row['question2']).lower().split()\n",
    "\tq2 = set(q2_list)\n",
    "\tq2words = q2.difference(stops)\n",
    "\tif len(q2words) == 0:\n",
    "\t\treturn '0:0:0:0:0:0:0:0'\n",
    "\n",
    "\twords_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "    \n",
    "\tq1stops = q1.intersection(stops)\n",
    "\tq2stops = q2.intersection(stops)\n",
    "\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\tshared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\tshared_words = q1words.intersection(q2words)\n",
    "\tshared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "\tq1_weights = [weights.get(w, 0) for w in q1words]\n",
    "\tq2_weights = [weights.get(w, 0) for w in q2words]\n",
    "\ttotal_weights = q1_weights + q1_weights\n",
    "\t\n",
    "\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "\tR2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "\tR31 = len(q1stops) / len(q1words) #stops in q1\n",
    "\tR32 = len(q2stops) / len(q2words) #stops in q2\n",
    "\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "\tRcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "\tif len(q1_2gram) + len(q2_2gram) == 0:\n",
    "\t\tR2gram = 0\n",
    "\telse:\n",
    "\t\tR2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n",
    "\n",
    "df_train =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "df_train = df_train.fillna(' ')\n",
    "\n",
    "df_test=  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "df_test = df_test.fillna(' ')\n",
    "\n",
    "# explore\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n",
    "\n",
    "words = [x for y in train_qs for x in y]\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "\n",
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']\n",
    "\t\n",
    "x['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "# x['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "x_train_question = x[:df_train.shape[0]]\n",
    "x_test_question  = x[df_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph features (Seb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate graph features for Quora question data. Features will be written in a csv file in path folder.\n",
    "Args:\n",
    "    path: folder containing train.csv and test.csv and to write csv features file.\n",
    "Return:\n",
    "\n",
    "\"\"\"\n",
    "train =pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "train = train.fillna(' ')\n",
    "\n",
    "test=  pd.read_csv('test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "test = test.fillna(' ')\n",
    "train = train.drop(['id','question1','question2', 'is_duplicate'], axis=1)\n",
    "test = test.drop(['id','question1','question2'], axis=1)\n",
    "train_test = pd.concat([train,test], ignore_index=True)\n",
    "\n",
    "# Create graph\n",
    "G=nx.Graph()\n",
    "\n",
    "edge_list = []\n",
    "for index, row in train_test.iterrows():\n",
    "    edge_list.append([train_test['qid1'][index],train_test['qid2'][index]])\n",
    "\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "print('Number of nodes:', G.number_of_nodes())\n",
    "print('Number of edges:', G.number_of_edges())\n",
    "\n",
    "\n",
    "train['q1_neigh'] = np.nan; train['q2_neigh'] = np.nan\n",
    "train['common_neigh'] = np.nan; train['distinct_neigh'] = np.nan\n",
    "#train['all_simple_paths_3'] =  np.nan #TOO LONG !\n",
    "train['clique_size'] = np.nan\n",
    "#train['number_of_clique'] = np.nan #TOO LONG !\n",
    "\n",
    "\n",
    "# Computing train features\n",
    "print('Computing train features')\n",
    "for index, row in tqdm(train.iterrows()):\n",
    "    neigh_1 = G.neighbors(train['qid1'][index])\n",
    "    neigh_2 = G.neighbors(train['qid2'][index])\n",
    "\n",
    "    train.loc[index,'q1_neigh'] = len(neigh_1)\n",
    "    train.loc[index,'q2_neigh'] = len(neigh_2)\n",
    "    train.loc[index,'common_neigh'] = len(list(nx.common_neighbors(G,train['qid1'][index],train['qid2'][index])))\n",
    "    train.loc[index,'distinct_neigh'] = len(neigh_1)+len(neigh_2)-len(list(nx.common_neighbors(G,train['qid1'][index],train['qid2'][index])))\n",
    "\n",
    "    #train.loc[index,'all_simple_paths_3'] = len(list(nx.all_simple_paths(G,train['qid1'][index],train['qid2'][index])))\n",
    "\n",
    "    train.loc[index,'clique_size'] = nx.node_clique_number(G,train['qid1'][index])\n",
    "    #train.loc[index,'number_of_clique'] = nx.number_of_cliques(G,train['qid1'][index])\n",
    "\n",
    "train = train.drop(['qid1','qid2'],axis=1)\n",
    "\n",
    "# print('Writing train features...')\n",
    "# \ttrain.to_csv(os.path.join(path,'train_graph_feat.csv'))\n",
    "\n",
    "print('Computing test features')\n",
    "for index, row in tqdm(test.iterrows()):\n",
    "    neigh_1 = G.neighbors(test['qid1'][index])\n",
    "    neigh_2 = G.neighbors(test['qid2'][index])\n",
    "\n",
    "    test.loc[index,'q1_neigh'] = len(neigh_1)\n",
    "    test.loc[index,'q2_neigh'] = len(neigh_2)\n",
    "    test.loc[index,'common_neigh'] = len(list(nx.common_neighbors(G,test['qid1'][index],test['qid2'][index])))\n",
    "    test.loc[index,'distinct_neigh'] = len(neigh_1)+len(neigh_2)-len(list(nx.common_neighbors(G,test['qid1'][index],test['qid2'][index])))\n",
    "\n",
    "    #test.loc[index,'all_simple_paths_3'] = len(list(nx.all_simple_paths(G,test['qid1'][index],test['qid2'][index])))\n",
    "\n",
    "    test.loc[index,'clique_size'] = nx.node_clique_number(G,test['qid1'][index])\n",
    "    #test.loc[index,'number_of_clique'] = nx.number_of_cliques(G,test['qid1'][index])\n",
    "\n",
    "test = test.drop(['qid1','qid2'],axis=1)\n",
    "\n",
    "#     print('Writing test features...')\t    \n",
    "# \ttest.to_csv(os.path.join(path,'test_graph_feat.csv'))\n",
    "\n",
    "# \tprint('CSV written ! see: ', path, \" | suffix: \", \"_graph_feat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N gram (seb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_2gram_feat = pd.read_csv('data/train_2gram_feat.csv', sep=',', encoding='latin-1')\n",
    "test_2gram_feat = pd.read_csv('data/test_2gram_feat.csv', sep=',', encoding='latin-1')\n",
    "train_3gram_feat = pd.read_csv('data/train_3gram_feat.csv', sep=',', encoding='latin-1')\n",
    "test_3gram_feat = pd.read_csv('data/test_3gram_feat.csv', sep=',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = pd.read_csv('data/train_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_test = pd.read_csv('data/test_features_glove.csv', sep=',', encoding='latin-1')\n",
    "features_train= features_train.drop(['question1', 'question2'], axis=1)\n",
    "features_test = features_test.drop(['id','qid1','qid2','question1', 'question2'], axis=1)\n",
    "data_train = pd.read_csv('train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "Y_train=data_train[\"is_duplicate\"].values\n",
    "features_test[\"q1_pr\"]=pagerank_feats_test[\"q1_pr\"]\n",
    "features_test[\"q2_pr\"]=pagerank_feats_test[\"q2_pr\"]\n",
    "features_train[\"q1_pr\"]=pagerank_feats_train[\"q1_pr\"]\n",
    "features_train[\"q2_pr\"]=pagerank_feats_train[\"q2_pr\"]\n",
    "features_test[\"q1_hash\"]=test_comb[\"q1_hash\"]\n",
    "features_test[\"q2_hash\"]=test_comb[\"q2_hash\"]\n",
    "features_test[\"q1_freq\"]=test_comb[\"q1_freq\"]\n",
    "features_test[\"q2_freq\"]=test_comb[\"q2_freq\"]\n",
    "features_train[\"q1_hash\"]=train_comb[\"q1_hash\"]\n",
    "features_train[\"q2_hash\"]=train_comb[\"q1_hash\"]\n",
    "features_train[\"q1_freq\"]=train_comb[\"q1_freq\"]\n",
    "features_train[\"q2_freq\"]=train_comb[\"q2_freq\"]\n",
    "features_train['q1_q2_intersect']=train_feat['q1_q2_intersect']\n",
    "features_test['q1_q2_intersect']=test_feat['q1_q2_intersect']\n",
    "features_train[\"core1\"]=train_cores[\"core1\"]\n",
    "features_train[\"core2\"]=train_cores[\"core2\"]\n",
    "features_train[\"core3\"]=train_cores[\"core3\"]\n",
    "features_test[\"core1\"]=test_cores[\"core1\"]\n",
    "features_test[\"core2\"]=test_cores[\"core2\"]\n",
    "features_test[\"core3\"]=test_cores[\"core3\"]\n",
    "features_train[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]=df_train_core\n",
    "features_test[['q1_kcores', 'q2_kcores', 'q1_q2_kcores_ratio', 'q1_q2_kcores_diff', 'q1_q2_kcores_diff_normed']]=df_test_core\n",
    "features_train[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=x_train[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "features_test[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]=x_test[['word_match','tfidf_wm','tfidf_wm_stops','jaccard','wc_diff','wc_ratio','wc_diff_unique','wc_ratio_unique','wc_diff_unq_stop','wc_ratio_unique_stop','same_start',\n",
    " 'char_diff','char_diff_unq_stop','total_unique_words','total_unq_words_stop','char_ratio']]\n",
    "\n",
    "features_train[['q1_neigh', 'q2_neigh', 'common_neigh', 'distinct_neigh', 'clique_size']]=train\n",
    "features_test[['q1_neigh', 'q2_neigh', 'common_neigh', 'distinct_neigh', 'clique_size']]=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]=x_train_question[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]\n",
    "features_test[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]=x_test_question[[ 'q1_how','q2_how','how_both','q1_what','q2_what','what_both','q1_which','q2_which','which_both','q1_who','q2_who','who_both','q1_where','q2_where','where_both','q1_when','q2_when','when_both','q1_why','q2_why','why_both','caps_count_q1','caps_count_q2','diff_caps','exactly_same']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]=train_2gram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "features_test[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]=test_2gram_feat[['bigram_coocurence','bigram_distinct','bigram_nostpwrd_coocurence','bigram_nostpwrd_distinct']]\n",
    "features_train[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]=train_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]\n",
    "features_test[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]=test_3gram_feat[['3gram_cooccurence','3gram_distinct','3gram_nostpwrd_cooccurence','3gram_nostpwrd_distinct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= features_train\n",
    "X_test = features_test\n",
    "X_train=X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_train=X_train.fillna(value=0)\n",
    "X_test=X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test=X_test.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in X_train.columns if x not in ['is_duplicate']]\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "cv_folds=5\n",
    "early_stopping_rounds=50\n",
    "# modelfit(xgb1, X_train, predictors)\n",
    "alg=xgb1\n",
    "dtrain=X_train.copy()\n",
    "xgb_param = alg.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain['is_duplicate'].values)\n",
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "    metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "#Fit the algorithm on the data\n",
    "alg.fit(dtrain[predictors], dtrain['is_duplicate'],eval_metric='auc')\n",
    "\n",
    "#Predict training set:\n",
    "dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "\n",
    "#Print model report:\n",
    "print (\"\\nModel Report\")\n",
    "print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['is_duplicate'].values, dtrain_predictions))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['is_duplicate'], dtrain_predprob))\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= features_train.drop(['question1','question2','is_duplicate','cosine_distance','jaccard_distance','euclidean_distance','norm_wmd','fuzz_WRatio','len_word_q2','len_word_q1','minkowski_distance','braycurtis_distance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test= features_test.drop(['qid1', 'id', 'qid2','question1','question2','cosine_distance','jaccard_distance','euclidean_distance','norm_wmd','fuzz_WRatio','len_word_q2','len_word_q1','minkowski_distance','braycurtis_distance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1 = xgb1.predict_proba(X_test1)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred1.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred1[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### light gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "NUM_FOLDS = 5\n",
    "RANDOM_SEED = 2017\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.zeros((len(X_test1), NUM_FOLDS))\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=X_train1.values\n",
    "X_test=X_test1.values\n",
    "for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train, Y_train)):\n",
    "    print('Fitting fold {fold_num + 1} of {kfold.n_splits}')\n",
    "    \n",
    "    print(len(ix_train))\n",
    "    print(X_train.shape)\n",
    "    X_fold_train = X_train[ix_train,:]\n",
    "    X_fold_val = X_train[ix_val,:]\n",
    "\n",
    "    y_fold_train = Y_train[ix_train]\n",
    "    y_fold_val = Y_train[ix_val]\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'feature_fraction': 0.486,\n",
    "        'num_leaves': 158,\n",
    "        'lambda_l2': 50,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_boost_round': 5000,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'verbose': 1,\n",
    "        'bagging_fraction_seed': RANDOM_SEED,\n",
    "        'feature_fraction_seed': RANDOM_SEED,\n",
    "    }\n",
    "    \n",
    "    lgb_data_train = lgb.Dataset(X_fold_train, y_fold_train)\n",
    "    lgb_data_val = lgb.Dataset(X_fold_val, y_fold_val)    \n",
    "    evals_result = {}\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        lgb_data_train,\n",
    "        valid_sets=[lgb_data_train, lgb_data_val],\n",
    "        evals_result=evals_result,\n",
    "        num_boost_round=lgb_params['num_boost_round'],\n",
    "        early_stopping_rounds=lgb_params['early_stopping_rounds'],\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    \n",
    "    fold_train_scores = evals_result['training'][lgb_params['metric']]\n",
    "    fold_val_scores = evals_result['valid_1'][lgb_params['metric']]\n",
    "    \n",
    "    print('Fold {}: {} rounds, training loss {:.6f}, validation loss {:.6f}'.format(\n",
    "        fold_num + 1,\n",
    "        len(fold_train_scores),\n",
    "        fold_train_scores[-1],\n",
    "        fold_val_scores[-1],\n",
    "    ))\n",
    "    print()\n",
    "    \n",
    "    cv_scores.append(fold_val_scores[-1])\n",
    "    y_test_pred[:, fold_num] = model.predict(X_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'column': list(X_train.columns),\n",
    "    'importance': model.feature_importance(),\n",
    "}).sort_values(by='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Final CV score:', final_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.mean(y_test_pred, axis=1)\n",
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_test.shape[0]):\n",
    "        f.write(str(i)+','+str(y_test[i])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train1, Y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "# UPDownSampling\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8 * len(pos_train))], neg_train))\n",
    "y_train = np.array(\n",
    "    [0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8 * len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "y_valid = np.array(\n",
    "    [0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    "# params['scale_pos_weight'] = 0.2\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'ujson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c080f6458562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\spacy\\cli\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\spacy\\cli\\download.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_package_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\spacy\\cli\\link.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msymlink_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath2str\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprints\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\spacy\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mftfy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mujson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'ujson'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
