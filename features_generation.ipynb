{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTEGRAD Challenge - Feature generation\n",
    "\n",
    "*Abderrahim AIT-AZZI, SÃ©bastien OHLEYER, Mickael SUTTON*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. WMD distance, Sent2vec, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMD Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = './glove.42B.300d.txt.word2vec'\n",
    "word_embedding_model_glove = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Amine/Desktop/MVA2017/ALTEGRAD/TP3/for moodle/code/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features for train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some useful features:\n",
    "\n",
    "    -Using the fuzz package to compute some ratio of string similarity between question 1 et question 2.\n",
    "    -The length of questions\n",
    "    -Word embedding of the questions and compute different distances: WMD, cosine, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "data_train = data_train.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding of the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=word_embedding_model_glove\n",
    "norm_model = word_embedding_model_glove\n",
    "norm_model.init_sims(replace=True)\n",
    "\n",
    "data_train['wmd'] = data_train.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data_train.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data_train.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data_train.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data_train.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_train['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_train['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data_train['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_train['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save features for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(question1_vectors, open('data/q1_glove.pkl', 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open('data/q2_glove.pkl', 'wb'), -1)\n",
    "\n",
    "data_train.to_csv('data/train_features_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generete features for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/test.csv', sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['len_q1'] = data_test.question1.apply(lambda x: len(str(x)))\n",
    "data_test['len_q2'] = data_test.question2.apply(lambda x: len(str(x)))\n",
    "data_test['diff_len'] = data_test.len_q1 - data_test.len_q2\n",
    "data_test['len_char_q1'] = data_test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_test['len_char_q2'] = data_test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_test['len_word_q1'] = data_test.question1.apply(lambda x: len(str(x).split()))\n",
    "data_test['len_word_q2'] = data_test.question2.apply(lambda x: len(str(x).split()))\n",
    "data_test['common_words'] = data_test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_test['fuzz_qratio'] = data_test.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_WRatio'] = data_test.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_ratio'] = data_test.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_token_set_ratio'] = data_test.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_partial_token_sort_ratio'] = data_test.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_token_set_ratio'] = data_test.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data_test['fuzz_token_sort_ratio'] = data_test.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_model = model\n",
    "norm_model.init_sims(replace=True)\n",
    "data_test['wmd'] = data_test.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data_test.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data_test.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data_test.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data_test.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data_test['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_test['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data_test['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data_test['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save features for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(question1_vectors, open('data/q1_glove_test.pkl', 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open('data/q2_glove_test.pkl', 'wb'), -1)\n",
    "data_test.to_csv('data/test_features_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.pagerank import generate_pagerank\n",
    "path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply to train...\n",
      "Apply to test...\n",
      "Main PR generator...\n",
      "Apply to train...\n",
      "Writing train...\n",
      "Apply to test...\n",
      "Writing test...\n",
      "CSV written !\n"
     ]
    }
   ],
   "source": [
    "generate_pagerank(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Question frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.question_freq import generate_question_freq\n",
    "path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train features...\n",
      "Writing test features...\n",
      "CSV written ! see:  ./data\n"
     ]
    }
   ],
   "source": [
    "generate_question_freq(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Intersection of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.question_inter import generate_question_inter\n",
    "path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train features...\n",
      "Writing test features...\n",
      "CSV written ! see:  ./data\n"
     ]
    }
   ],
   "source": [
    "generate_question_inter(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. K cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.kcores import generate_kcores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100279/100279 [00:00<00:00, 190659.69it/s]\n",
      "100%|ââââââââââ| 100279/100279 [00:01<00:00, 86552.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train features...\n",
      "Writing test features...\n",
      "CSV written ! see:  ./data  | suffix:  _kcores.csv\n"
     ]
    }
   ],
   "source": [
    "generate_kcores(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.question_kcores import generate_question_kcores\n",
    "path='./data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]/Users/sebastienohleyer/Documents/ENS MVA/alte/features_engineering/question_kcores.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['kcores'][df.question.isin(ck)] = k\n",
      "100%|ââââââââââ| 29/29 [00:21<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train features...\n",
      "Writing test features...\n",
      "CSV written ! see:  ./data  | suffix:  _question_kcores.csv\n"
     ]
    }
   ],
   "source": [
    "generate_question_kcores(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.tfidf import generate_tfidf\n",
    "path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Features\n",
      "world_match\n",
      "tfidf\n",
      "tfidf_wm_stops\n",
      "jaccard, wc_diff; wc_ratio, wc_diff_unique, wc_ratio_unique\n",
      "wc_diff_unq_stop, wc_ratio_unique_stop\n",
      "same_start, char_diff\n",
      "char_diff_unq_stop\n",
      "total_unique_words\n",
      "total_unq_words_stop\n",
      "char_ratio\n",
      "world_match\n",
      "tfidf\n",
      "tfidf_wm_stops\n",
      "jaccard, wc_diff; wc_ratio, wc_diff_unique, wc_ratio_unique\n",
      "wc_diff_unq_stop, wc_ratio_unique_stop\n",
      "same_start, char_diff\n",
      "char_diff_unq_stop\n",
      "total_unique_words\n",
      "total_unq_words_stop\n",
      "char_ratio\n",
      "Writing train features...\n",
      "Writing test features...\n",
      "CSV written ! see:  ./data/  | suffix:  _tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "generate_tfidf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features_engineering.graph_features import generate_graph_features\n",
    "path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 115.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 58940\n",
      "Number of edges: 100279\n",
      "Computing train features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80100it [09:02, 147.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 69.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20179it [03:59, 84.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test features...\n",
      "CSV written ! see:  ./data  | suffix:  _graph_feat.csv\n"
     ]
    }
   ],
   "source": [
    "generate_graph_features(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features_engineering.cooccurence_distinct_ngram import generate_cooccurence_distinct_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 75.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying to train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80100it [18:10, 73.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 103.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying to test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20179it [02:46, 121.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test features...\n",
      "CSV written ! see:  ./data  | suffix:  _3gram_feat.csv\n"
     ]
    }
   ],
   "source": [
    "path = './data'\n",
    "generate_cooccurence_distinct_ngram(path,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "(\"name 'weights' is not defined\", 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-79bec5236120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_shares'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_shares\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4875\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4877\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4878\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4879\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4971\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4972\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4973\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4974\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4975\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-79bec5236120>\u001b[0m in \u001b[0;36mword_shares\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mshared_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mshared_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshared_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mq1_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq1words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mq2_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq2words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtotal_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq1_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-79bec5236120>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mshared_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mshared_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshared_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mq1_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq1words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mq2_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq2words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtotal_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq1_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: (\"name 'weights' is not defined\", 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "path = './data'\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
    "    \n",
    "def word_shares(row):\n",
    "\tq1_list = str(row['question1']).lower().split()\n",
    "\tq1 = set(q1_list)\n",
    "\tq1words = q1.difference(stops)\n",
    "\tif len(q1words) == 0:\n",
    "\t\treturn '0:0:0:0:0:0:0:0'\n",
    "        \n",
    "\tq2_list = str(row['question2']).lower().split()\n",
    "\tq2 = set(q2_list)\n",
    "\tq2words = q2.difference(stops)\n",
    "\tif len(q2words) == 0:\n",
    "\t\treturn '0:0:0:0:0:0:0:0'\n",
    "\n",
    "\twords_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "\tq1stops = q1.intersection(stops)\n",
    "\tq2stops = q2.intersection(stops)\n",
    "\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\tshared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\tshared_words = q1words.intersection(q2words)\n",
    "\tshared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "\tq1_weights = [weights.get(w, 0) for w in q1words]\n",
    "\tq2_weights = [weights.get(w, 0) for w in q2words]\n",
    "\ttotal_weights = q1_weights + q1_weights\n",
    "\t\n",
    "\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "\tR2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "\tR31 = len(q1stops) / len(q1words) #stops in q1\n",
    "\tR32 = len(q2stops) / len(q2words) #stops in q2\n",
    "\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "\tRcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "\tif len(q1_2gram) + len(q2_2gram) == 0:\n",
    "\t\tR2gram = 0\n",
    "\telse:\n",
    "\t\tR2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n",
    "\n",
    "df_train =pd.read_csv(os.path.join(path,'train.csv'), sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\",\"is_duplicate\"])\n",
    "df_train = df_train.fillna(' ')\n",
    "\n",
    "df_test=  pd.read_csv(os.path.join(path,'test.csv'), sep=',',names = [\"id\", \"qid1\", \"qid2\", \"question1\",\"question2\"])\n",
    "df_test = df_test.fillna(' ')\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "\n",
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']\n",
    "\t\n",
    "x['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "x['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "x_train_question = x[:df_train.shape[0]]\n",
    "x_test_question  = x[df_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
